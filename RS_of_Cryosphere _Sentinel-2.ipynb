{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6741df-8785-4bf6-9bb8-6a0330529fe8",
   "metadata": {},
   "source": [
    "# 5. Sentinel-2 application\n",
    "## Neural Net Mapping of Hudson Bay Sea Ice\n",
    "\n",
    "The Canadian Ice Service produces weekly regional sea ice charts for ship safety and environmental monitoring. In this project I use a convolutional neural network to automatically generate ice charts from satellite imagery. With increasing availability of satellite data, this network may be able to produce similar ice charts globally at higher detail.\n",
    "\n",
    "-  Collected 3392 satellite images of Hudson Bay sea ice in the Canadian Arctic from 2016-1-1 to 2018-7-31\n",
    "-  Generated sea ice concentrations masks for each image using Canadian Regional Ice Chart shapefiles\n",
    "-  Trained a Convolutional Neural Network (U-Net) to generate sea ice charts from satellite images based on eight different classes (seven levels of ice concentration and land)\n",
    "    -  Model Accuracy: 83%\n",
    "    -  Model Mean IoU (intersection over union) score: 0.44\n",
    "- Found a strong class imbalance favoring thick solid ice due to complete freezing in the winter months. Future work should focus on collecting more data during the spring months when ice is thawing and there is a greater variety in ice concentration.\n",
    "- Future work could also take advantage of additional satellite wavelength collection bands beyond the visible spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b83d9-cfff-4b98-bff6-d0e40d2a357b",
   "metadata": {},
   "source": [
    "# 1. Data Collection\n",
    "\n",
    "There are two main data sources for this project: Sentinel-2 satellite images and Canadian Regional Ice Charts. These were used to generate images and masks, respectively.\n",
    "\n",
    "## 1.1 Sentinel-2 \n",
    "\n",
    "The Sentinel-2 mission is made up of a pair of satellites that image the globe roughly every 5 days. They capture 12 optical bands including the visible spectrum. Bands 3, 4, and 8 were used for this project, representing near infra-red, red and green wavelengths. Sentinelhub provides a python API for acquiring Sentinel-2 images.\n",
    "\n",
    "## 1.2 Canadian Regional Ice Charts\n",
    "\n",
    "Canadian Regional Ice Charts show geospatial sea ice concentrations for ship safety and environmental monitoring. They are produced weekly on Mondays by the Canadian Ice Service for five large regions:\n",
    "\n",
    "- Hudson Bay\n",
    "- Western Arctic\n",
    "- Eastern Arctic\n",
    "- Eastern Coast\n",
    "- Great Lakes\n",
    "\n",
    "This project investigated the Hudson Bay region. A sample ice chart for Hudson Bay on April 12, 2021 is shown below. Each region on the chart has a corresponding set of codes giving information on (among other things) the concentration of sea ice. The table below shows the codes corresponding to ice concentration. All charts are archived and available as shapefiles from the National Snow and Ice Data Centre dating back to 2006.\n",
    "\n",
    "| <img src=\"Data/Sentinel-2/Images/Ice_Chart_ex.gif\" width=\"600\" />  | <img src=\"Data/Sentinel-2/Images/Chart_Codes.PNG\" height=\"400\" /> |  \n",
    "|:--:|:--:| \n",
    "| *Sample Ice Chart for Hudson Bay* | *SIGRID-3 Ice Chart Codes* |\n",
    "\n",
    "## 1.3 Data Collection Workflow\n",
    "\n",
    "Data was collected using the EO-Learn python library, which provides a framework for slicing large geographical areas into smaller, more manageable tiles called EOPatches. \n",
    "\n",
    "| <img src=\"Data/Sentinel-2/Images/Region-Grid.png\" width=\"600\" />   |  \n",
    "|:--:|\n",
    "| *Sliced hudson bay region. Image/mask pairs are generated on each tile.* |\n",
    "\n",
    "After slicing the region, an EO-Learn workflow was developed to aquire satellite images through the Sentinelhub API. The workflow includes filtering steps to remove cloudy images and a custom step to add a time-dependent image mask (from ice chart chapefiles). The data collection workflow loops over each EOPatch and consists of:\n",
    "\n",
    "- **add_data:** Collect all available satellite images for the EOPatch in false color (bands B03, B04, and B08)\n",
    "- **remove_dates:** Discard images that were taken more than 36 hours away from an available ice chart\n",
    "- **add_valid_mask:** Collect a mask for each image that says which pixels are valid data\n",
    "- **add_coverage:** Collect a mask for each image that says which pixels are blocked by clouds\n",
    "- **remove_cloudy_scenes:** Remove images where the sum of cloudy and non-valid pixels is greater than 5%\n",
    "- **time_raster:** Custom task to locate the ice chart temporally closest to the image, locate the area of the chart associated with the image, and rasterize into an ice concentration mask for the image\n",
    "- **save_im:** Save each image and mask \n",
    "\n",
    "| <img src=\"Data/Sentinel-2/Images/image-mask.png\" width=\"600\" />     |  \n",
    "|:--:|\n",
    "| *Image and mask pair generated through the EO-Learn workflow* |\n",
    "\n",
    "# 2 Data Processing\n",
    "\n",
    "## 2.1 Class Definitions\n",
    "\n",
    "In order to simplify analysis, the 31 SIGRID-3 classes shown in section 1.2 were binned into 8 classes broadly defined as:\n",
    "\n",
    "- 0: <10% ice\n",
    "- 1: 10-30% ice\n",
    "- 2: 30-50% ice\n",
    "- 3: 50-70% ice\n",
    "- 4: 70-90% ice\n",
    "- 5: 90-100% ice\n",
    "- 6: fast ice (thick ice that is 'fastened' to the coastline)\n",
    "- 7: land\n",
    "\n",
    "With these definitions, the pixel-wise distribution of classes across all 3,392 images in the dataset was calculated. There is a strong class imbalance, with open water, 90-100% ice, and land occupying most of the dataset.\n",
    "\n",
    "|<img src=\"Data/Sentinel-2/Images/class_dist.png\" width=\"400\" /> |\n",
    "|:--:|\n",
    "| *Pixel-wise class distribution over all images* |\n",
    "\n",
    "## 2.2 Data Input Pipeline\n",
    "\n",
    "Before being fed into the model for training, the following operations were performed on the dataset:\n",
    "\n",
    "- Image/mask pairs were split into training (80%) and validation (20%) sets\n",
    "    - The split was stratified based the most common class represented in the images\n",
    "- Within the training data, images with high amounts of under-represented pixels were over-sampled (eg. duplicated in the training set to increase their weight)\n",
    "    - This helped address the class imbalance in the dataset that would skew a model towards the over-represented classes\n",
    "- Random image augmentation:\n",
    "    - Random flip left-right\n",
    "    - Random flip up-down\n",
    "    - Random image rotation by +- 5 degrees (corners we mapped to black in the image and land in the mask)\n",
    "\n",
    "The result is a stream of image/mask pairs like this:\n",
    "\n",
    "|<img src=\"Data/Sentinel-2/Images/input_image_mask.png\" width=\"600\" /> |\n",
    "|:--:|\n",
    "| *Sample image/mask pair training data. Note the random image rotation.* |\n",
    "\n",
    "# 3 Model Building\n",
    "\n",
    "The goal of the model was to automatically generate a sea ice chart based on a satellite image. This is an image segmentation problem, wherein a model is expected to predict a class for each pixel in an image. \n",
    "\n",
    "## 3.1 U-Net\n",
    "\n",
    "A popular convolutional neural network architecture for image segmentation is the 'U-Net'. It consists of a contraction path (composed of successive convolution, ReLU activation, and max pooling operations) followed by an expansion path. In the expansion path, a combination of up-sampling and concatenation with high resolution images from the contraction path allows the network to localize features of the image at higher and higher resolution until each pixel of the image has a predicted class. A diagram of the basic architecture of the network is shown below.\n",
    "\n",
    "|<img src=\"Data/Sentinel-2/Images/U-Net.png\" width=\"600\" />  |\n",
    "|:--:|\n",
    "| *Base U-Net Architecture. Source: [https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf)* |\n",
    "\n",
    "## 3.2 Model Definition\n",
    "\n",
    "The model for this project is an adapted version of a U-NET from the Dstl Satellite Imagery Feature Detection Kaggle competition. That competition also aimed to classify pixels in satellite images, so this model architecture was expected to be good fit here too. See [here](https://www.kaggle.com/drn01z3/end-to-end-baseline-with-u-net-keras) for the original model writeup. The base model architecture was supplemented with dropout layers to help with over-fitting. A diagram of the final model architecture is shown below.\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"Data/Sentinel-2/Images/model-map.png\" width=\"400\" /> \n",
    "</p>\n",
    "\n",
    "## 3.3 Training and Predictions\n",
    "\n",
    "The neural network above was trained for 100 epochs (where an epoch is a run through the entire training dataset). Plots of training/validation loss and mean IoU metric are shown below. IoU is also known as the [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index).\n",
    "\n",
    "|<img src=\"Data/Sentinel-2/Images/train-val.png\" width=\"600\" />   |\n",
    "|:--:|\n",
    "| *Model performance over training epochs* |\n",
    "\n",
    "The lowest validation data loss is achieved after roughly 50 epochs, after which the model begins to over-train. The model weights at this 'optimal' point were saved and used for the final model. A confusion matrix with these weights is below. The model is very good at preicting open water and land, and somewhat poorer at predicting the intermediate ice concentrations (10-90%). This could almost certainly be improved by collecting more images with these intermediate ice concentrations, which would be best achieved by focusing data collection on the springtime/early summer months when the ice is thawing.\n",
    "\n",
    "|<img src=\"Data/Sentinel-2/Images/confusion_matrix.png\" width=\"600\" />   |\n",
    "|:--:|\n",
    "| *Final model confusion matrix for validation data* |\n",
    "\n",
    "A series of validation data images, true masks, and predicted masks are shown below. The class imbalance of the dataset is apparent here, with land and solid ice dominating many of the images. Nevertheless the model is able to provide a good prediction of localized ice concentration in many cases. It is also interesting to note that the model provides finer detail than the published ice charts. With additional training data, perhaps algorith such as this could be used to develop finer-detailed ice charts than are curently available. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e1d7e-e037-46ee-b124-28a9885627cb",
   "metadata": {},
   "source": [
    "Here, we will collect and save satellite images over the Hudson Bay Region. It includes data cleaning wherin images are filtered based on data availability, cloud cover, and date. The workflow also generates a sea-ice concentration mask for each image using downloaded Canadian Regional Sea Ice Charts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec7a36-ca29-478d-98ee-6edcd38bdde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0ebea-1675-4cdc-adc8-6154bbce8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image \n",
    "\n",
    "# # set the necessary directories\n",
    "# img_dir = 'Data/Images/*.jpg'\n",
    "\n",
    "# img_filenames = os.listdir(img_dir)\n",
    "# img_names = [s.split('.')[0] for s in img_filenames]\n",
    "\n",
    "# img_ext = '.jpg'\n",
    "# print(np.size(img_names))\n",
    "img_dir_all = 'Data/Sentinel-2/Data/Images/*.jpg'\n",
    "img_dir = 'Data/Sentinel-2/Data/Images/'\n",
    "# Use glob to get the list of filenames\n",
    "img_filenames = glob.glob(img_dir_all)\n",
    "\n",
    "# Extract the base names without extensions\n",
    "img_names = [os.path.basename(s).split('.')[0] for s in img_filenames]\n",
    "img_ext = '.jpg'\n",
    "# Print the number of images\n",
    "print(np.size(img_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154056f6-e976-4c53-ab4f-eb131a18eda7",
   "metadata": {},
   "source": [
    "## Defining Masks\n",
    "Masks are encoded in SIGRID-3 format. See here for more information: https://library.wmo.int/doc_num.php?explnum_id=9270\n",
    "We will map the ice concentratoin codes according to the following library to simplify the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d0f162-0ae4-4db0-8a11-0eea4ede1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_lib = {55:0, #ice free\n",
    "1:0, #<1/10 (open water)\n",
    "2:0, #bergy water\n",
    "10:1, #1/10\n",
    "12:1, #1/10-2/10\n",
    "13:1, #1/10-3/10\n",
    "20:1, #2/20\n",
    "23:1, #2/20-3/10\n",
    "24:2, #2/20-4/10\n",
    "30:2, #...\n",
    "34:2,\n",
    "35:2,\n",
    "40:2,\n",
    "45:2,\n",
    "46:3,\n",
    "50:3,\n",
    "56:3,\n",
    "57:3,\n",
    "60:3,\n",
    "67:3,\n",
    "68:4, #...\n",
    "70:4, #7/10\n",
    "78:4, #7/10-8/10\n",
    "79:4, #7/10-9/10\n",
    "80:4, #8/10\n",
    "89:4, #8/10-9/10\n",
    "81:5, #8/10-10/10\n",
    "90:5, #9/10\n",
    "91:5, #9/10-10/10\n",
    "92:6, #10/10 - fast ice\n",
    "100:7, #land\n",
    "99:7, #unknown - there is nothing in this class for this dataset\n",
    "}\n",
    "#define a colormap for the mask\n",
    "n_colors=8\n",
    "ice_colors = n_colors-1\n",
    "jet = plt.get_cmap('jet', ice_colors)\n",
    "newcolors = jet(np.linspace(0, 1, ice_colors))\n",
    "black = np.array([[0, 0, 0, 1]])\n",
    "white = np.array([[1, 1, 1, 1]])\n",
    "newcolors = np.concatenate((newcolors, black), axis=0) #land will be black\n",
    "cmap = ListedColormap(newcolors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c795ab5-720f-4ed2-affd-6aee078748ab",
   "metadata": {},
   "source": [
    "Run through and convert all masks from SIGRID-3 to simplified. Also store the pixel class counts for each mask in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56557b-8751-4768-8e86-24a959b306d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to map mask values according to above library\n",
    "def map_mask(mask, lib):\n",
    "    new_mask = mask.copy()\n",
    "    for key, val in lib.items():#map the elements of the array to their new values according to the library\n",
    "        new_mask[mask==key]=val\n",
    "    return new_mask\n",
    "\n",
    "#function to calculate the value counts over all pixels in an image (fed in as a numpy array)\n",
    "def bincount_2d(arr, max_int):\n",
    "    counts_full = [0 for n in range(max_int)]\n",
    "    for row in arr:\n",
    "        counts = np.bincount(row).tolist()#get the counts for the row\n",
    "        pad = [0 for n in range(max_int-len(counts))]\n",
    "        counts = counts + pad #add extra zeroes to account for colors above the max in the row\n",
    "        counts_full = [counts_full[n] + counts[n] for n in range(max_int)]\n",
    "    return(counts_full)\n",
    "    \n",
    "# convert all mask files from SIGRID 3 format to simplified\n",
    "mask_dir = 'Data/Sentinel-2/Data/Masks/'\n",
    "mask_ext = '-mask.png'\n",
    "new_mask_ext = '-mask-mod.png'\n",
    "dat = []#list that will hold information on the masks\n",
    "for img_name in img_names:\n",
    "    name = mask_dir + img_name\n",
    "    # importing the image\n",
    "    if os.path.exists(name + mask_ext):\n",
    "        mask = Image.open(name + mask_ext)\n",
    "\n",
    "        # converting mask\n",
    "        mask = np.array(mask)#convert to numpy\n",
    "        new_mask = map_mask(mask, mask_lib)#map values\n",
    "    \n",
    "        #update dataframe\n",
    "        name = img_name.split('-')  \n",
    "        d = [img_name, \n",
    "             name[0][1:],  #patch id\n",
    "             name[1][0:4], #year\n",
    "             name[1][4:6], #month\n",
    "             name[1][6:8], #day\n",
    "             name[1][8:10]]#hour\n",
    "    \n",
    "        counts = bincount_2d(new_mask, n_colors) #values counts of the class of ice over all pixels in the image\n",
    "        d.extend(counts)\n",
    "        dat.append(d)\n",
    "    \n",
    "        # exporting the image \n",
    "        new_mask = Image.fromarray(new_mask)#convert back to image\n",
    "        new_mask.save('Data/Sentinel-2/Data/Newmask/' + img_name + new_mask_ext, 'PNG')\n",
    "\n",
    "mask_dir = 'Data/Sentinel-2/Data/Newmask/'#update mask directory and extension\n",
    "mask_ext= new_mask_ext\n",
    "\n",
    "#create dataframe of mask information\n",
    "mask_df = pd.DataFrame(dat, columns = ['name', 'patch_id', 'year', 'month', 'day', 'hour', \n",
    "                            'conc_0', 'conc_1', 'conc_2', 'conc_3', 'conc_4', 'conc_5', 'conc_6',  \n",
    "                            'conc_land'])\n",
    "\n",
    "#plot realtive frequency of ice concentrations in images\n",
    "counts = mask_df.iloc[:,6:].sum()\n",
    "norm = counts.sum()\n",
    "probs = counts/norm*100\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "probs.plot(kind='bar')\n",
    "plt.ylabel('Fraction of Pixel Values (%)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854afd5b-26d2-4fbd-b443-960991de4c44",
   "metadata": {},
   "source": [
    "We see that classes for 0%, 90%, and 100% ice concentration and the land class make up most of the pixels. To deal with this class imbalance in our model, we will over-sample images that contain more than 30% of the minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d28cb0f-ee37-49bc-9719-4c2bee08e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_df['conc_minor']=mask_df[['conc_1', #lists the total concentration of under-represented ice classes\n",
    "                               'conc_2', \n",
    "                               'conc_3', \n",
    "                               'conc_4', \n",
    "                              ]].sum(axis=1)\n",
    "\n",
    "n_pixels = mask_df.iloc[0, 6:].sum(axis=0)#total number of pixels in each image\n",
    "over_sample_names = mask_df[mask_df['conc_minor']/n_pixels>0.3] #we will over-sample these images of the under-represented classes\n",
    "over_sample_names = over_sample_names['name'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec23ff4-f844-4295-9cbe-0ccd526e7ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fce40c-c0fe-4ee4-9d94-ee7896da2b06",
   "metadata": {},
   "source": [
    "## Tensorflow Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694604fd-ca32-4807-8735-39f4cdae0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_max = mask_df.iloc[:,6:-1].idxmax(axis=1) #category of the most common class in the image. We will stratify our train test split by this\n",
    "class_max.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdabe19-617f-4c00-a55d-8b51a1ed5182",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_addons scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377a348-2d45-4c57-a6d2-ff35158c046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b903dd9c-cb9b-45a3-8543-723cfcd52e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59efbe8-9e9b-4d2e-b353-e8d7a8cb0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names_copy = img_names.copy()\n",
    "#img_names=img_names[0:1000] #I slice the img_names list to match the dataframe\n",
    "print(np.size(img_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efcf9b-9d40-4492-a4f2-229dab7000d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.size(img_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e394b838-a400-4c71-b778-50cc97a1aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# import tensorflow_addons as tfa  # REMOVED\n",
    "\n",
    "# Simple replacement for tfa.image.rotate\n",
    "def rotate_image(image, angle, fill_mode='constant', fill_value=0):\n",
    "    \"\"\"\n",
    "    Simple replacement for tfa.image.rotate using native TensorFlow\n",
    "    \"\"\"\n",
    "    # Convert angle and create transformation matrix\n",
    "    cos_angle = tf.cos(angle)\n",
    "    sin_angle = tf.sin(angle)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    height = tf.cast(tf.shape(image)[0], tf.float32)\n",
    "    width = tf.cast(tf.shape(image)[1], tf.float32)\n",
    "    \n",
    "    # Center coordinates\n",
    "    cx, cy = width / 2.0, height / 2.0\n",
    "    \n",
    "    # Rotation matrix around center\n",
    "    transform = [\n",
    "        cos_angle, -sin_angle, cx * (1 - cos_angle) + cy * sin_angle,\n",
    "        sin_angle, cos_angle, cy * (1 - cos_angle) - cx * sin_angle,\n",
    "        0.0, 0.0\n",
    "    ]\n",
    "    \n",
    "    # Apply transformation\n",
    "    rotated = tf.raw_ops.ImageProjectiveTransformV3(\n",
    "        images=tf.expand_dims(image, 0),\n",
    "        transforms=tf.reshape(transform, [1, 8]),\n",
    "        output_shape=tf.shape(image)[:2],\n",
    "        fill_mode=fill_mode.upper(),\n",
    "        fill_value=fill_value,\n",
    "        interpolation='BILINEAR' if len(tf.shape(image)) == 3 else 'NEAREST'\n",
    "    )\n",
    "    \n",
    "    return tf.squeeze(rotated, 0)\n",
    "\n",
    "# pick which images we will use for testing and which for validation\n",
    "names = mask_df['name'].values\n",
    "train_names, validation_names, train_max, validation_max = train_test_split(img_names, class_max, \n",
    "                                                                            train_size=0.8, test_size=0.2, \n",
    "                                                                            random_state=0, stratify=class_max)\n",
    "#add over-sampled images to the train dataset\n",
    "train_over_sample_names = np.array([name for name in train_names if name in over_sample_names])\n",
    "N_over_sample = int(len(train_names)/1.5) #number of additional samples to add\n",
    "ids = np.arange(len(train_over_sample_names))\n",
    "choices = np.random.choice(ids, N_over_sample)#an additional set of images to add on to the train names\n",
    "add_train_names = train_over_sample_names[choices].tolist()\n",
    "train_names.extend(add_train_names)\n",
    "\n",
    "IMG_SIZE = (256, 256)\n",
    "\n",
    "#function to read image and mask from file\n",
    "def read_image(image_name):\n",
    "    image = tf.io.read_file(img_dir + image_name + img_ext)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    mask = tf.io.read_file(mask_dir + image_name + mask_ext)\n",
    "    mask = tf.image.decode_image(mask, channels=1, expand_animations=False)\n",
    "    mask = tf.image.resize(mask, IMG_SIZE)\n",
    "    mask = tf.cast(mask, tf.uint8)\n",
    "    return image, mask\n",
    "\n",
    "import random\n",
    "\n",
    "#image augmentation function to randomly flip and rotate each image and corresponding mask\n",
    "def augment_image(image, mask):\n",
    "    n = tf.random.uniform([], 0,1)\n",
    "    if n<0.5: \n",
    "        image = tf.image.flip_left_right(image)\n",
    "        mask = tf.image.flip_left_right(mask)\n",
    "        \n",
    "    n = tf.random.uniform([], 0,1)\n",
    "    if n<0.5: \n",
    "        image = tf.image.flip_up_down(image)\n",
    "        mask = tf.image.flip_up_down(mask)\n",
    "    \n",
    "    #rotate image randomly in the range of +-5 degrees\n",
    "    n = tf.random.uniform([], -1,1)\n",
    "    image = rotate_image(image, np.pi/36*n, fill_mode='constant', fill_value=0) #add black to rotated corners\n",
    "    mask = rotate_image(mask, np.pi/36*n, fill_mode='constant', fill_value=7) #make this black space correspond to land\n",
    "    return image, mask\n",
    "\n",
    "TRAIN_LENGTH = int(len(train_names))\n",
    "VAL_LENGTH = int(len(validation_names))\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((train_names))#read filenames\n",
    "ds_train = ds_train.map(read_image, num_parallel_calls=tf.data.AUTOTUNE) #convert filenames to stream of images/masks\n",
    "ds_train = ds_train.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE) #convert filenames to stream of images/masks\n",
    "train_dataset = ds_train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((validation_names))#read filenames\n",
    "ds_val = ds_val.map(read_image) #convert filenames to stream of images/masks\n",
    "val_dataset = ds_val.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a2094-9271-4a41-a616-24e790c3805e",
   "metadata": {},
   "source": [
    "## Display Sample Image and Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea19bce6-8431-4998-ab83-2fc73cbdd39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols = len(display_list), figsize=(15, 6))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        axs[i].set_title(title[i])\n",
    "        if i==0:\n",
    "            axs[i].imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        else:\n",
    "            msk = axs[i].imshow(display_list[i], cmap = cmap, vmin=0, vmax=n_colors-1)\n",
    "        axs[i].axis('off')\n",
    "        \n",
    "    #plot colorbar\n",
    "    cbar = fig.colorbar(msk, ax=axs, location='right')\n",
    "    tick_locs = (np.arange(n_colors) + 0.5)*(n_colors-1)/n_colors#new tick locations so they are in the middle of the colorbar\n",
    "    cbar.set_ticks(tick_locs)\n",
    "    cbar.set_ticklabels(np.arange(n_colors))\n",
    "    plt.show()\n",
    "\n",
    "for image, mask in ds_train.take(11):\n",
    "    sample_image, sample_mask = image, mask\n",
    "display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b42cee-279f-48b5-97d3-0e7904c71731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_train_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a6072-4dac-473e-a37f-462da35669b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for add_train_names in add_train_names:\n",
    "    name = mask_dir + add_train_names\n",
    "    # importing the image\n",
    "    if os.path.exists(name + mask_ext):\n",
    "        mask = Image.open(name + mask_ext)\n",
    "        # converting mask\n",
    "        mask = np.array(mask)#convert to numpy\n",
    "        new_mask = map_mask(mask, mask_lib)#map values\n",
    "        #update dataframe\n",
    "        name = img_name.split('-')\n",
    "        d = [img_name,\n",
    "             name[0][1:], #patch id\n",
    "             name[1][0:4], #year\n",
    "             name[1][4:6], #month\n",
    "             name[1][6:8], #day\n",
    "             name[1][8:10]]#hour\n",
    "        counts = bincount_2d(new_mask, n_colors) #values counts of the class of ice over all pixels in the image\n",
    "        d.extend(counts)\n",
    "        dat.append(d)\n",
    "        # exporting the image\n",
    "        new_mask = Image.fromarray(new_mask) #convert back to image\n",
    "        new_mask.save('Data/Sentinel-2/Data/Newmask/' + img_name + new_mask_ext, 'PNG')\n",
    "\n",
    "mask_dir = 'Data/Sentinel-2/Data/Newmask/'#update mask directory and extension\n",
    "mask_ext= new_mask_ext\n",
    "#create dataframe of mask information\n",
    "mask_df = pd.DataFrame(dat, columns = ['name', 'patch_id', 'year', 'month', 'day', 'hour',\n",
    "'conc_0', 'conc_1', 'conc_2', 'conc_3', 'conc_4', 'conc_5', 'conc_6',\n",
    "'conc_land'])\n",
    "#plot realtive frequency of ice concentrations in images\n",
    "counts = mask_df.iloc[:,6:].sum()\n",
    "norm = counts.sum()\n",
    "probs = counts/norm*100\n",
    "plt.figure(figsize=(8,5))\n",
    "probs.plot(kind='bar')\n",
    "plt.ylabel('Fraction of Pixel Values (%)')\n",
    "plt.grid()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b77b2f-6272-4b61-b16f-ac6740d3699b",
   "metadata": {},
   "source": [
    "## Display Sample Image and Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2521881-df82-41d6-8771-4d78a5c21500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "print(PIL.__version__)\n",
    "from keras.preprocessing.image import array_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22ab40c-941e-46a2-aa30-42d7fdc63b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols = len(display_list), figsize=(15, 6))\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "    for i in range(len(display_list)):\n",
    "        axs[i].set_title(title[i])\n",
    "        if i==0:\n",
    "            axs[i].imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        else:\n",
    "            msk = axs[i].imshow(display_list[i], cmap = cmap, vmin=0, vmax=n_colors-1)\n",
    "            axs[i].axis('off')\n",
    "    #plot colorbar\n",
    "    cbar = fig.colorbar(msk, ax=axs, location='right')\n",
    "    tick_locs = (np.arange(n_colors) + 0.5)*(n_colors-1)/n_colors#new tick locations so they are in the middle of the colorbar\n",
    "    cbar.set_ticks(tick_locs)\n",
    "    cbar.set_ticklabels(np.arange(n_colors))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf115d-213e-4d9f-bd54-1a6de3494559",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, mask in ds_train.take(20):\n",
    "    sample_image, sample_mask = image, mask\n",
    "    display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e2fdb4-842d-4b13-9f3c-03e78eda2f2d",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "This model is an adapted version U-NET from the Dstl Satellite Imagery Feature Detection Kaggle competition. That competition also aimed to classify pixels in satelite images, so this model architucture might be a good fit here too. https://www.kaggle.com/drn01z3/end-to-end-baseline-with-u-net-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5265a-5669-4fc9-b944-1ec468fd4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbdad7-1051-4612-97b3-0c904d9b4fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
    "from tensorflow.keras import Model\n",
    "def get_unet():\n",
    "    inputs = Input(shape=[IMG_SIZE[0], IMG_SIZE[1], 3])\n",
    "    conv1 = Conv2D(32, 3, 1, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, 3, 1, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    drop1 = Dropout(0.5)(pool1)\n",
    "\n",
    "    conv2 = Conv2D(64, 3, 1, activation='relu', padding='same')(drop1)\n",
    "    conv2 = Conv2D(64, 3, 1, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    drop2 = Dropout(0.5)(pool2)\n",
    "\n",
    "    conv3 = Conv2D(128, 3, 1, activation='relu', padding='same')(drop2)\n",
    "    conv3 = Conv2D(128, 3, 1, activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    drop3 = Dropout(0.5)(pool3)\n",
    "\n",
    "    conv4 = Conv2D(256, 3, 1, activation='relu', padding='same')(drop3)\n",
    "    conv4 = Conv2D(256, 3, 1, activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    drop4 = Dropout(0.5)(pool4)\n",
    "\n",
    "    conv5 = Conv2D(512, 3, 1, activation='relu', padding='same')(drop4)\n",
    "    conv5 = Conv2D(512, 3, 1, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = Conv2D(256, 3, activation = 'relu', padding = 'same')(UpSampling2D(size=(2, 2))(conv5))\n",
    "    merge6 = concatenate([up6, conv4], axis=3)\n",
    "    drop6 = Dropout(0.5)(merge6)\n",
    "    conv6 = Conv2D(256, 3, 1, activation='relu', padding='same')(drop6)\n",
    "    conv6 = Conv2D(256, 3, 1, activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up7 = Conv2D(128, 3, activation = 'relu', padding = 'same')(UpSampling2D(size=(2, 2))(conv6))\n",
    "    merge7 = concatenate([up7, conv3], axis=3)\n",
    "    drop7 = Dropout(0.5)(merge7)\n",
    "    conv7 = Conv2D(128, 3, 1, activation='relu', padding='same')(drop7)\n",
    "    conv7 = Conv2D(128, 3, 1, activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    up8 = Conv2D(64, 3, activation = 'relu', padding = 'same')(UpSampling2D(size=(2, 2))(conv7))\n",
    "    merge8 = concatenate([up8, conv2], axis=3)\n",
    "    drop8 = Dropout(0.5)(merge8)\n",
    "    conv8 = Conv2D(64, 3, 1, activation='relu', padding='same')(drop8)\n",
    "    conv8 = Conv2D(64, 3, 1, activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    up9 = Conv2D(32, 3, activation = 'relu', padding = 'same')(UpSampling2D(size=(2, 2))(conv8))\n",
    "    merge9 = concatenate([up9, conv1], axis=3)\n",
    "    drop9 = Dropout(0.5)(merge9)\n",
    "    conv9 = Conv2D(32, 3, 1, activation='relu', padding='same')(drop9)\n",
    "    conv9 = Conv2D(32, 3, 1, activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(n_colors, 1, 1, activation='softmax')(conv9) #softmax converts the output to a list of probabilities that must sum to 1\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    return model\n",
    "\n",
    "model = get_unet() \n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4c0ef-a35f-4a11-8ebc-bb3f385505d8",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716b25b-dcbe-43fd-af79-d987628e63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate a mask from the model predictions\n",
    "def create_mask(pred_mask, ele=0):\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)#use the highest proabbaility class as the prediction\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[ele]\n",
    "\n",
    "#helper functions to plot image, mask, and predicted mask while training\n",
    "def show_predictions(dataset=None, num=1, ele=0):\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            display([image[ele], mask[ele], create_mask(pred_mask, ele)])\n",
    "    else:\n",
    "        display([sample_image, sample_mask, create_mask(model.predict(sample_image[tf.newaxis, ...]))])\n",
    "\n",
    "#function to display loss during training\n",
    "def plot_loss_acc(loss, val_loss, epoch):#, acc, val_acc, epoch):\n",
    "    \n",
    "    epochs = range(epoch+1)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,5))\n",
    "\n",
    "    ax.plot(epochs, loss, 'r', label='Training loss')\n",
    "    ax.plot(epochs, val_loss, 'bo', label='Validation loss')\n",
    "    ax.set_title('Training and Validation Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss Value')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "#callback to clear output and show predictions\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        self.loss.append(logs['loss'])\n",
    "        self.val_loss.append(logs['val_loss'])\n",
    "        \n",
    "        show_predictions()\n",
    "        plot_loss_acc(self.loss, self.val_loss, epoch)\n",
    "        \n",
    "#callback to reduce learning rate when loss plateaus\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=8, verbose=1,)\n",
    "\n",
    "#Define IoU metric (by stack overflow user HuckleberryFinn)\n",
    "class UpdatedMeanIoU(tf.keras.metrics.MeanIoU):\n",
    "    def __init__(self,\n",
    "               y_true=None,\n",
    "               y_pred=None,\n",
    "               num_classes=None,\n",
    "               name=None,\n",
    "               dtype=None):\n",
    "        super(UpdatedMeanIoU, self).__init__(num_classes = num_classes,name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.math.argmax(y_pred, axis=-1)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "checkpoint_path = \"Data/Sentinel-2/training/cp-{epoch:04d}.keras\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    # save_weights_only=True,\n",
    "    save_freq=5*BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555728f-13c1-4cb5-8063-0672c1ccf674",
   "metadata": {},
   "source": [
    "## First try - default settings with 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af694c98-f524-46be-bb3b-4fe102ae6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model first try\n",
    "model=get_unet()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['sparse_categorical_accuracy', UpdatedMeanIoU(num_classes=n_colors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb9800-d093-4dd6-9879-f146f5c43e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "VAL_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = VAL_LENGTH//BATCH_SIZE//VAL_SUBSPLITS\n",
    "\n",
    "model_history = model.fit(train_dataset, \n",
    "                          epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=val_dataset,\n",
    "                          callbacks=[DisplayCallback(), lr_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120f028-9b0c-4725-b106-417f23e2cf43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ee0a0-2450-4846-aaed-87ceda2b57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "epochs = range(EPOCHS)\n",
    "ax[0].plot(epochs, loss, 'r', label='Training')\n",
    "ax[0].plot(epochs, val_loss, 'bo', label='Validation')\n",
    "ax[0].set_title('Training and Validation Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss Value')\n",
    "ax[0].legend()\n",
    "IoU_key = list(model_history.history.keys())[2]\n",
    "acc = model_history.history[IoU_key]\n",
    "val_acc = model_history.history['val_'+IoU_key]\n",
    "ax[1].plot(epochs, acc, 'r', label='Training')\n",
    "ax[1].plot(epochs, val_acc, 'bo', label='Validation')\n",
    "ax[1].set_title('Training and Validation IoU')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('IoU Value')\n",
    "ax[1].legend()\n",
    "#plt.show()\n",
    "plt.savefig('10E_sparse_categorical_accuracy.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53b22c-6155-43fb-8685-430b6cca57a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9795a2d-5c7b-42fe-b87c-d194b55bfd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights for checkpoint 4\n",
    "# print(os.listdir(checkpoint_dir))\n",
    "model.load_weights(checkpoint_dir + '/cp-0005.keras')\n",
    "scores = model.evaluate(val_dataset, verbose=0)\n",
    "print('Final Model Validation Scores')\n",
    "print('Loss: {:.3f}'.format(scores[0]))\n",
    "print('Accuracy: {:.3f}'.format(scores[1]))\n",
    "print('IoU: {:.3f}'.format(scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85b238-6b9c-4d9f-af3b-37ddc1404bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26c4c3-4ce5-4b20-ae07-3e5f0523c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a confusion matrix for the first try\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def get_cm(model, val_ds):\n",
    "    cm = np.zeros((8,8))\n",
    "    for img_batch, mask_batch in val_dataset:\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        pred_batch = model.predict(img_batch)\n",
    "        pred_batch = tf.argmax(pred_batch, axis=-1)#take the highest probability as the prediction for each pixel\n",
    "        for n, pred in enumerate(pred_batch):\n",
    "            pred = np.array(pred).flatten() #flattened array of predicted pixels for each image\n",
    "            mask = np.array(mask_batch[n, ...]).flatten() #flattened array of mask pixels for the image\n",
    "            y_pred.extend(pred)\n",
    "            y_true.extend(mask)\n",
    "        cm = cm + confusion_matrix(y_true, y_pred)\n",
    "    return cm\n",
    "\n",
    "cm = get_cm(model, val_dataset)\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(cm.astype(int), annot=True, fmt=\"d\")\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7b86e-ab11-4fbe-9851-26eb0a0e8c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef601b1-09e8-4daa-8cc8-aba1c68555ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the confusion matrix by rows (actual values)\n",
    "cm_row_normalized = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "# Plot row-normalized confusion matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm_row_normalized, annot=True, fmt=\".2%\", cmap=\"YlGnBu\")\n",
    "plt.title('Row-Normalized Confusion Matrix (Rows sum to 100%)')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df902c97-a03f-4d09-aa5c-658d1514f19a",
   "metadata": {},
   "source": [
    "## Different metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c1c33-4e2b-4b64-a223-a5b477e29d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# train model second try with only 10 epochs and 1000 images and with different metrics\n",
    "smooth = 1e-12\n",
    "\n",
    "def jaccard_coef(y_true, y_pred):\n",
    "    # **author** = Vladimir Iglovikov (modified for sparse categorical)\n",
    "    # Convert sparse labels to one-hot for Jaccard calculation\n",
    "    y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=n_colors)\n",
    "    y_true_one_hot = tf.squeeze(y_true_one_hot, axis=-2)  # Remove extra dimension\n",
    "    \n",
    "    intersection = K.sum(y_true_one_hot * y_pred, axis=[0, 1, 2])\n",
    "    sum_ = K.sum(y_true_one_hot + y_pred, axis=[0, 1, 2])\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    # **author** = Vladimir Iglovikov (modified for sparse categorical)\n",
    "    y_pred_pos = tf.nn.softmax(y_pred)  # Apply softmax to get probabilities\n",
    "    y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=n_colors)\n",
    "    y_true_one_hot = tf.squeeze(y_true_one_hot, axis=-2)  # Remove extra dimension\n",
    "    \n",
    "    intersection = K.sum(y_true_one_hot * y_pred_pos, axis=[0, 1, 2])\n",
    "    sum_ = K.sum(y_true_one_hot + y_pred_pos, axis=[0, 1, 2])\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return K.mean(jac)\n",
    "\n",
    "# Alternative: Use built-in IoU metric (recommended)\n",
    "def mean_iou_metric(y_true, y_pred):\n",
    "    # Convert predictions to class predictions\n",
    "    y_pred_classes = tf.argmax(y_pred, axis=-1)\n",
    "    y_true_squeeze = tf.squeeze(y_true, axis=-1)\n",
    "    \n",
    "    # Use TensorFlow's built-in MeanIoU\n",
    "    m = tf.keras.metrics.MeanIoU(num_classes=n_colors)\n",
    "    m.update_state(y_true_squeeze, y_pred_classes)\n",
    "    return m.result()\n",
    "\n",
    "model = get_unet()\n",
    "\n",
    "# Option 1: Use custom Jaccard metrics\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "    metrics=[jaccard_coef, jaccard_coef_int, 'sparse_categorical_accuracy']\n",
    ")\n",
    "\n",
    "EPOCHS = 10\n",
    "VAL_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = VAL_LENGTH//BATCH_SIZE//VAL_SUBSPLITS\n",
    "\n",
    "model_history = model.fit(train_dataset,\n",
    "                          epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=val_dataset,\n",
    "                          callbacks=[DisplayCallback(), lr_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f402472-600d-4d0a-ac7b-84cdf4a08eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "epochs = range(EPOCHS)\n",
    "ax[0].plot(epochs, loss, 'r', label='Training')\n",
    "ax[0].plot(epochs, val_loss, 'bo', label='Validation')\n",
    "ax[0].set_title('Training and Validation Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss Value')\n",
    "ax[0].legend()\n",
    "IoU_key = list(model_history.history.keys())[2]\n",
    "acc = model_history.history[IoU_key]\n",
    "val_acc = model_history.history['val_'+IoU_key]\n",
    "ax[1].plot(epochs, acc, 'r', label='Training')\n",
    "ax[1].plot(epochs, val_acc, 'bo', label='Validation')\n",
    "ax[1].set_title('Training and Validation IoU')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('IoU Value')\n",
    "ax[1].legend()\n",
    "#plt.show()\n",
    "plt.savefig('10E_Jaccard_coef.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657c944-7ef7-47d5-8ead-ad44d64527f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights for checkpoint 5\n",
    "# print(os.listdir(checkpoint_dir))\n",
    "model.load_weights(checkpoint_dir + '/cp-0005.ckpt')\n",
    "scores = model.evaluate(val_dataset, verbose=0)\n",
    "print('Final Model Validation Scores')\n",
    "print('Loss: {:.3f}'.format(scores[0]))\n",
    "print('Accuracy: {:.3f}'.format(scores[1]))\n",
    "print('IoU: {:.3f}'.format(scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aedf7f6-1bf1-4dbd-9843-9983e5aa1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(val_dataset, num=10, ele=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38dcb5-875f-4481-bbe9-1b79f10faf88",
   "metadata": {},
   "source": [
    "## Different loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac5ff6-bc65-4fdc-9990-403b917047d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model with mean squared error as loss function\n",
    "model=get_unet()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['sparse_categorical_accuracy', UpdatedMeanIoU(num_classes=n_colors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4f4f0-7f92-496e-bd9a-a22fe5c8f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "VAL_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = VAL_LENGTH//BATCH_SIZE//VAL_SUBSPLITS\n",
    "\n",
    "model_history = model.fit(train_dataset,\n",
    "                          epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=val_dataset,\n",
    "                          callbacks=[DisplayCallback(), lr_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd173f74-686b-4b34-9238-9f0e778fc2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "epochs = range(EPOCHS)\n",
    "ax[0].plot(epochs, loss, 'r', label='Training')\n",
    "ax[0].plot(epochs, val_loss, 'bo', label='Validation')\n",
    "ax[0].set_title('Training and Validation Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss Value')\n",
    "ax[0].legend()\n",
    "IoU_key = list(model_history.history.keys())[2]\n",
    "acc = model_history.history[IoU_key]\n",
    "val_acc = model_history.history['val_'+IoU_key]\n",
    "ax[1].plot(epochs, acc, 'r', label='Training')\n",
    "ax[1].plot(epochs, val_acc, 'bo', label='Validation')\n",
    "ax[1].set_title('Training and Validation IoU')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('IoU Value')\n",
    "ax[1].legend()\n",
    "#plt.show()\n",
    "plt.savefig('10E_Jaccard_coef.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194c89e-2d8c-4af6-94f7-0ea351118253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
