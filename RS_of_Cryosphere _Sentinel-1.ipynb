{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483a19a2-9052-4b77-b603-e38ddd48b7cc",
   "metadata": {},
   "source": [
    "# 2. Aquifer Detection in Greenland and Antarctica Using C-Band Active Systems\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The polar ice sheets of Antarctica and Greenland are undergoing significant changes due to global climate change. One key process influencing these ice sheets is the formation of subglacial water storage areas, known as Perennial Firn Aquifers (PFAs). These aquifers store liquid water in the subsurface firn (snow that has persisted through one or more melting seasons). PFAs play a crucial role in regulating the thermal state of the ice and may impact the overall stability of ice sheets through processes like meltwater-induced hydrofracturing. Understanding PFAs is vital because, as global temperatures rise, the increase in meltwater production across polar regions becomes a significant factor in ice shelf disintegration.\n",
    "\n",
    "PFAs are typically found in areas with both high melt rates and high snowfall, which allows the water to accumulate below the surface. While they were first discovered in mountain glaciers, scientists have also identified PFAs in Greenland and Svalbard. Several techniques have been used to study PFAs, such as ground and airborne radar, and satellite remote sensing data, particularly using Sentinel-1’s C-band radar imagery. In Antarctica, recent research has started to explore the presence of PFAs through field-based studies and Ground Penetrating Radar (GPR), uncovering aquifers in areas like the Wilkins Ice Shelf.\n",
    "\n",
    "This project will use C-band active systems to detect aquifers in both Greenland and Antarctica. \n",
    "Sentinel-1 (S1) synthetic aperture radar (SAR) has the ability to detect PFA: the detection mechanism relies on the strong absorption of the S1 radar signal by liquid water, potentially at the aquifer water table (when close to the surface; e.g., less than 10 m), but more likely within the firn layer above it. The limited penetration depth at C-band does not always allow sensing the top of the water table itself, which on average, in Greenland, is located 22 m below the surface. However, the time-delayed increase in radar backscatter caused by the delayed refreezing of water in the firn at aquifer locations yields a unique signature in the S1 backscatter data. The long-term continuity in C-band SAR observations with the S1 constellation missions warrants the monitoring of firn aquifers at the decadal time scale.\n",
    "\n",
    "The S1 detected PFAs will be compared with Operation IceBridge (OIB) data and regional climate models to validate the results. \n",
    "\n",
    "We will follow a series of steps to detect and map these aquifers. While the provided code focuses on the Antarctic Peninsula, your task will be to apply the same approach to Greenland using the dataset provided (collected with the same instruments as presented below). Finally, you will be required to validate your results and discuss your findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc1922-c05f-4b44-a8d6-075b2ef2b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a818bb9-bead-4c20-b951-38297fbc6d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this block, the AP mask is imported with the same resolution/grid as the Sentinel stacks (approximately 4 km),\n",
    "# and both the mask and the Lat and Lon matrices are reconstructed, which are useful for georeferencing the stacks as well\n",
    "# (in the 3031 reference system).\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from pyproj import Transformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Open the GeoTIFF using rasterio\n",
    "with rasterio.open('Data/Sentinel-1/maskAP.tif') as dataset:\n",
    "    # Read the mask data\n",
    "    maskAP = dataset.read(1)  # Reads the first (and only) layer of the GeoTIFF file\n",
    "    \n",
    "    # Get the transformation values to convert from pixel coordinates to geographic coordinates\n",
    "    transform_affine = dataset.transform\n",
    "    \n",
    "    # Extract the x and y coordinate values (in the units of the original projection, e.g., meters in EPSG:3031)\n",
    "    cols, rows = np.meshgrid(np.arange(maskAP.shape[1]), np.arange(maskAP.shape[0]))\n",
    "    xs, ys = rasterio.transform.xy(transform_affine, rows, cols)\n",
    "\n",
    "    # Flatten the coordinate arrays (from 2D to 1D)\n",
    "    xs_flat = np.array(xs).flatten()\n",
    "    ys_flat = np.array(ys).flatten()\n",
    "\n",
    "# Convert the x and y coordinates into geographic coordinates (latitude and longitude)\n",
    "# We use pyproj to transform the coordinates from EPSG:3031 (Antarctic Polar Stereographic) to EPSG:4326 (lat/lon)\n",
    "\n",
    "    transformer = Transformer.from_crs(\"EPSG:3031\", \"EPSG:4326\", always_xy=True)\n",
    "    lon_flat, lat_flat = transformer.transform(xs_flat, ys_flat)\n",
    "\n",
    "    # Reshape the latitude and longitude arrays (from 1D to 2D)\n",
    "    lon = np.array(lon_flat).reshape(maskAP.shape)\n",
    "    lat = np.array(lat_flat).reshape(maskAP.shape)\n",
    "\n",
    "# Plot the mask as a function of latitude and longitude coordinates\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pcolormesh(lon, lat, maskAP, cmap='gray', shading='auto')\n",
    "plt.colorbar(label='Mask Value')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Antarctic Peninsula Mask (maskAP)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21c130f-5827-4ea7-b6c5-7514b057af7a",
   "metadata": {},
   "source": [
    "### Satellite data description\n",
    "\n",
    "C-band (5.4 GHz) radar backscatter measurements from Sentinel-1A and Sentinel-1B satellites in extra-wide swath mode are provided for this project. These measurements, collected in dual polarization (horizontal-horizontal and horizontal-vertical), feature a swath width of 410 km and a spatial resolution of 20 by 40 m.\n",
    "\n",
    "Using the Google Earth Engine Python API, standard preprocessing steps have been performed, including noise removal, radiometric calibration, and terrain correction. The data were aggregated and projected onto a 1 km global grid to enhance the signal-to-noise ratio, particularly for cross-polarized measurements.\n",
    "\n",
    "Each satellite (S1A and S1B) has a 12-day repeat cycle, with adjustments made for varying incidence and azimuth angles. Datasets were processed separately. This workflow has facilitated reliable backscatter measurements for analyzing firn aquifers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76246ed3-9b0a-4e46-b5f2-42b7e14447d5",
   "metadata": {},
   "source": [
    "Now we start by loading the necessary data files, which include georeferenced `.tif` images and associated `.csv` files containing relevant metadata or additional data points. Here's a breakdown of what happens:\n",
    "\n",
    "1. **Data Collection**: The program searches for all `.tif` and `.csv` files in a predefined folder (`data/`). Using the `glob` function, it locates the file paths for the TIFF images (which contain remote sensing data) and CSV files (likely containing complementary data, such as geographic coordinates or validation information).\n",
    "   \n",
    "2. **TIFF Loading**: Each `.tif` file is then opened using `rasterio`, a library that preserves important geospatial information such as Coordinate Reference System (CRS) and the affine transformation needed to map the images to real-world coordinates. The images are read into a list as 3D NumPy arrays, representing multiple bands (layers) of the data. The geospatial metadata for each file is also stored separately.\n",
    "\n",
    "3. **Data Stacking**: Once the `.tif` files are loaded, the individual layers (bands) from each stack are separated and merged into a single comprehensive stack. This stack will form the basis for further analysis. Each 2D image (a single band) is treated as one layer in this new 3D array.\n",
    "\n",
    "The final output is a single 3D array containing all the layers (bands) from all `.tif` files, which is ready to be processed for aquifer detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bf5f5-338c-4059-b44d-696afc79885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of data files (.tif and .csv)\n",
    "# Define the path to the folder\n",
    "data_dir = 'Data/Sentinel-1/data/'\n",
    "\n",
    "# Create two separate lists for .tif and .csv files\n",
    "tif_files = []\n",
    "csv_files = []\n",
    "\n",
    "# Use glob to find .tif and .csv files\n",
    "tif_files = glob.glob(os.path.join(data_dir, '*.tif'))\n",
    "csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n",
    "\n",
    "# Print the number of files and their names\n",
    "print(f\"Found {len(tif_files)} TIFF files:\")\n",
    "for tif in tif_files:\n",
    "    print(tif)\n",
    "\n",
    "print(f\"\\nFound {len(csv_files)} CSV files:\")\n",
    "for csv in csv_files:\n",
    "    print(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70797c-9380-4a05-b510-ecd77b20d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all images from the various tif files in a single stack: \n",
    "# STEP 1: load the tif files\n",
    "\n",
    "# List to hold image stacks (each .tif file contains multiple layers)\n",
    "image_data = []\n",
    "\n",
    "# List to hold georeferencing information\n",
    "geo_data = []\n",
    "\n",
    "# Loop through each .tif file\n",
    "for tif_file in tif_files:\n",
    "    # Open the .tif file using rasterio, which preserves georeferencing\n",
    "    with rasterio.open(tif_file) as src:\n",
    "        # Read all the layers (bands) in the stack as a NumPy array\n",
    "        img_stack = src.read()  # This returns a 3D array (bands, rows, cols)\n",
    "        \n",
    "        # Store the array of the image stack (do not try to convert to a unified array yet)\n",
    "        image_data.append(img_stack)\n",
    "        \n",
    "        # Store the georeferencing info (such as CRS and affine transform)\n",
    "        geo_data.append({\n",
    "            \"crs\": src.crs,\n",
    "            \"transform\": src.transform,\n",
    "            \"bounds\": src.bounds\n",
    "        })\n",
    "\n",
    "# Output the number of .tif files loaded and their shapes\n",
    "for idx, img in enumerate(image_data):\n",
    "    print(f\"File {idx+1}: {img.shape} (bands, rows, cols)\")\n",
    "\n",
    "print(f\"Loaded {len(image_data)} .tif files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f80fd1-87ef-4cb9-ac0f-fd8b5fd74639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: merge into a single stack\n",
    "\n",
    "# Assuming image_data is a list where each element is a NumPy array of shape (bands, 313, 322)\n",
    "\n",
    "# Initialize an empty list to hold all the individual images\n",
    "all_images = []\n",
    "\n",
    "# Loop through each image stack in image_data\n",
    "for img_stack in image_data:\n",
    "    # For each stack, split the images (along the first dimension)\n",
    "    for i in range(img_stack.shape[0]):\n",
    "        # Append each individual image (313x322) to the all_images list\n",
    "        all_images.append(img_stack[i])\n",
    "\n",
    "# Convert the list of images to a single NumPy array (stacked along the first dimension)\n",
    "all_images_stack = np.array(all_images)\n",
    "\n",
    "# Print the new dimensions of the stack\n",
    "print(f\"New stack dimensions: {all_images_stack.shape} (total images, rows, cols)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f96b3e-02e3-45df-8288-41c302fab91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dates contained in the csv files are imported and combined into a single date vector. The dates (initially in chronological order by revolution number)\n",
    "# are reordered chronologically, and using the same criteria, the images are reordered by extracting the indices that correspond to the ordered dates.\n",
    "\n",
    "# Initialize an empty list to hold individual DataFrames and dates\n",
    "dataframes = []\n",
    "date_list = []\n",
    "\n",
    "# Loop through each CSV file and read it into a DataFrame\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)  # Read the CSV file\n",
    "    dataframes.append(df)  # Append the DataFrame to the list\n",
    "    \n",
    "    # Extract the date column and convert to a list\n",
    "    if 'date' in df.columns:\n",
    "        date_list.extend(df['date'].tolist())  # Add dates to the date list\n",
    "\n",
    "# Convert date_list to a pandas Series for sorting\n",
    "date_series = pd.Series(date_list)\n",
    "\n",
    "# Sort the dates and get the sorted indices\n",
    "sorted_indices = date_series.argsort()\n",
    "\n",
    "# Sort the date list\n",
    "sorted_dates = date_series.sort_values().reset_index(drop=True)\n",
    "\n",
    "# Assuming all_images_stack is a NumPy array with shape (3780, 313, 322)\n",
    "# Reorder the image stack based on the sorted indices\n",
    "sorted_image_stack = all_images_stack[sorted_indices]\n",
    "\n",
    "# Print the sorted dates and the new stack dimensions\n",
    "print(\"Sorted Dates:\")\n",
    "print(sorted_dates)\n",
    "\n",
    "print(f\"New stack dimensions: {sorted_image_stack.shape} (total images, rows, cols)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2dd420-e548-4038-88b5-bc2b91269172",
   "metadata": {},
   "source": [
    "## Backscatter analysis \n",
    "\n",
    "In this section, we perform a series of tasks to analyze the backscatter time series for selected pixels on the Antarctic Peninsula:\n",
    "\n",
    "1. **Pixel Selection**: We define the geographic coordinates (latitude and longitude) for three specific pixels of interest. The goal is to track and analyze the backscatter values at these points over time.\n",
    "\n",
    "2. **Finding Nearest Pixels**: Using a function that computes the Euclidean distance between the target coordinates and all points in the latitude/longitude grid, the program identifies the nearest corresponding pixels in the dataset.\n",
    "\n",
    "3. **Extract Time Series**: After locating the pixels, we extract the backscatter time series from the image stack for each of the three selected pixels. These values are stored and indexed by date.\n",
    "\n",
    "4. **Smoothing the Data**: To reduce noise and provide clearer trends, a 3-week moving average is applied to the time series data for each pixel. This helps in visualizing broader patterns in the backscatter changes over time.\n",
    "\n",
    "5. **Mapping the Pixels**: Using the Basemap library, we plot the locations of the selected pixels on a map of the Antarctic Peninsula. This step provides a visual reference for where the pixels are located geographically.\n",
    "\n",
    "6. **Plotting the Time Series**: Finally, the smoothed time series for each pixel is plotted on a graph. This shows the variations in backscatter values over time, giving insights into the dynamic changes at these points.\n",
    "\n",
    "This block provides both spatial and temporal analysis, linking geographic locations with backscatter time series, essential for understanding aquifer dynamics in the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78481c-44c8-48da-b6b3-03a1efc5cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36239499-dfee-4b14-a706-e7458273d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Input data: Coordinates of the three selected pixels\n",
    "pixel1_coords = (-70, -65)  # Latitude, Longitude of pixel1\n",
    "pixel2_coords = (-70.90, -72.80)  # Latitude, Longitude of pixel2\n",
    "pixel3_coords = (-70.90, -71.70)  # Latitude, Longitude of pixel3\n",
    "\n",
    "# Function to find the nearest pixel in the latitude/longitude grid\n",
    "def find_closest_pixel(lat_grid, lon_grid, target_lat, target_lon):\n",
    "    \"\"\"\n",
    "    Find the closest pixel in the latitude/longitude grid to the target coordinates.\n",
    "    Calculates the Euclidean distance between the target point and all grid points,\n",
    "    then returns the index of the nearest pixel.\n",
    "    \"\"\"\n",
    "    # Compute the distance between each grid point and the target point\n",
    "    dist = np.sqrt((lat_grid - target_lat)**2 + (lon_grid - target_lon)**2)\n",
    "    # Get the index of the pixel with the smallest distance\n",
    "    idx = np.unravel_index(np.argmin(dist), lat_grid.shape)\n",
    "    return idx\n",
    "\n",
    "# Find the nearest pixel for pixel1, pixel2, and pixel3\n",
    "pixel1_idx = find_closest_pixel(lat, lon, *pixel1_coords)\n",
    "pixel2_idx = find_closest_pixel(lat, lon, *pixel2_coords)\n",
    "pixel3_idx = find_closest_pixel(lat, lon, *pixel3_coords)\n",
    "\n",
    "# Extract the time series for the selected pixels from the image stack\n",
    "time_series_pixel1 = sorted_image_stack[:, pixel1_idx[0], pixel1_idx[1]]\n",
    "time_series_pixel2 = sorted_image_stack[:, pixel2_idx[0], pixel2_idx[1]]\n",
    "time_series_pixel3 = sorted_image_stack[:, pixel3_idx[0], pixel3_idx[1]]\n",
    "\n",
    "# Convert sorted_dates to datetime format if it's not already\n",
    "if isinstance(sorted_dates[0], str):\n",
    "    sorted_dates_ts = [datetime.strptime(date, '%Y-%m-%d') for date in sorted_dates]\n",
    "\n",
    "# Create a pandas DataFrame to handle the time series data and apply the moving average\n",
    "df = pd.DataFrame({\n",
    "    'Date': sorted_dates_ts,\n",
    "    'Pixel1': time_series_pixel1,\n",
    "    'Pixel2': time_series_pixel2,\n",
    "    'Pixel3': time_series_pixel3\n",
    "})\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Apply a moving average of 3 weeks (assuming dates are weekly or spaced similarly)\n",
    "df_smoothed = df.rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "# Plot the positions of the three pixels on the map of the Antarctic Peninsula\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Use Plate Carrée projection (cylindrical) for clearer, less distorted projection\n",
    "m = Basemap(projection='cyl', llcrnrlat=-78, urcrnrlat=-58, llcrnrlon=-80, urcrnrlon=-50, resolution='i')\n",
    "\n",
    "# Draw coastlines, parallels, and meridians\n",
    "m.drawcoastlines()\n",
    "m.drawparallels(np.arange(-90, -60, 5), labels=[1, 0, 0, 0])\n",
    "m.drawmeridians(np.arange(-180, 180, 10), labels=[0, 0, 0, 1])\n",
    "\n",
    "# Convert lat/lon of the three pixels to map coordinates\n",
    "x1, y1 = m(pixel1_coords[1], pixel1_coords[0])  # Pixel1 lon, lat\n",
    "x2, y2 = m(pixel2_coords[1], pixel2_coords[0])  # Pixel2 lon, lat\n",
    "x3, y3 = m(pixel3_coords[1], pixel3_coords[0])  # Pixel3 lon, lat\n",
    "\n",
    "# Plot the locations of pixel1, pixel2, and pixel3\n",
    "m.scatter(x1, y1, color='red', label=f'Pixel1: {pixel1_coords}', s=100, marker='o')\n",
    "m.scatter(x2, y2, color='blue', label=f'Pixel2: {pixel2_coords}', s=100, marker='o')\n",
    "m.scatter(x3, y3, color='green', label=f'Pixel3: {pixel3_coords}', s=100, marker='o')\n",
    "\n",
    "# Add legend to the map\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# Add a title to the map\n",
    "plt.title('Locations of Selected Pixels on Antarctic Peninsula')\n",
    "\n",
    "# Show the map plot\n",
    "plt.show()\n",
    "\n",
    "# Now, plot the smoothed time series for all three pixels\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot time series for Pixel 1 with dashed lines between points\n",
    "plt.plot(df_smoothed.index, df_smoothed['Pixel1'], label=f'Pixel1 {pixel1_coords}', marker='o', color='red', linestyle='--')\n",
    "\n",
    "# Plot time series for Pixel 2 with dashed lines between points\n",
    "plt.plot(df_smoothed.index, df_smoothed['Pixel2'], label=f'Pixel2 {pixel2_coords}', marker='o', color='blue', linestyle='--')\n",
    "\n",
    "# Plot time series for Pixel 3 with dashed lines between points\n",
    "plt.plot(df_smoothed.index, df_smoothed['Pixel3'], label=f'Pixel3 {pixel3_coords}', marker='o', color='green', linestyle='--')\n",
    "\n",
    "# Add title, labels, and legend\n",
    "plt.title('Smoothed Time Series of Backscatter for Selected Pixels')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Backscatter Value (dB)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Rotate date labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the time series plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea2df96-7647-4d08-8020-6b0b954df0d1",
   "metadata": {},
   "source": [
    "This section divides the full image stack into annual sub-stacks based on the Antarctic year, which runs from July 1st of one year to June 30th of the following year.\n",
    "\n",
    "1. **Date Conversion**: We begin by converting the `sorted_dates` list into `Timestamps`, which allows for easier manipulation and filtering of dates. These dates represent when each image in the stack was captured.\n",
    "\n",
    "2. **Antarctic Year Definition**: The Antarctic year is defined from July 1st of one year to June 30th of the next. Based on the full range of dates in the dataset, we determine the start and end years.\n",
    "\n",
    "3. **Sub-stack Creation**: For each Antarctic year, a date range is created (e.g., July 1st, 2020 to June 30th, 2021). Using this range, a mask is applied to filter the images and their corresponding dates that fall within that period.\n",
    "\n",
    "4. **Storing Sub-stacks**: For each Antarctic year, a sub-stack of images and dates is stored in a dictionary. Each sub-stack contains only the images and dates corresponding to that year's date range.\n",
    "\n",
    "5. **Verification**: After the sub-stacks are created, the shapes of each sub-stack are printed to verify that the division is correct and that each sub-stack contains the expected number of images and date entries.\n",
    "\n",
    "This process is crucial for analyzing the seasonal dynamics in the Antarctic Peninsula, allowing us to study changes in backscatter and aquifer presence across different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d21c0a1-4b4e-41ee-8c12-c6a0730900db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The images are divided by Antarctic years from 01/07/yyyy to 30/06/yyyy+1\n",
    "\n",
    "# Assuming sorted_dates is a list or a Series of date strings, convert to Timestamps\n",
    "sorted_dates = pd.to_datetime(sorted_dates)  # Convert to Timestamps if it's a Series or list\n",
    "\n",
    "# Define the start and end year based on the date range\n",
    "start_year = sorted_dates.dt.year.min()  # Get the minimum year\n",
    "end_year = sorted_dates.dt.year.max()  # Get the maximum year\n",
    "\n",
    "# Initialize a dictionary to hold sub-stacks for each year\n",
    "annual_sub_stacks = {}\n",
    "\n",
    "# Generate date ranges from 1/7/20xx to 30/6/20yy\n",
    "for year in range(start_year, end_year + 1):\n",
    "    start_date = pd.Timestamp(year=year, month=7, day=1)\n",
    "    end_date = pd.Timestamp(year=year + 1, month=6, day=30)\n",
    "\n",
    "    # Create a mask to filter images based on the current date range\n",
    "    mask = (sorted_dates >= start_date) & (sorted_dates <= end_date)\n",
    "    \n",
    "    # Check if the mask covers the entire range\n",
    "    if mask.any() and (sorted_dates[mask].min() == start_date) and (sorted_dates[mask].max() == end_date):\n",
    "        # Create sub-stacks for both images and dates\n",
    "        annual_sub_stacks[f\"{start_date.date()} - {end_date.date()}\"] = {\n",
    "            \"images\": all_images_stack[mask.values],\n",
    "            \"dates\": sorted_dates[mask].values  # Store the corresponding dates\n",
    "        }\n",
    "\n",
    "# Output the shape of each sub-stack to verify\n",
    "for period, stack in annual_sub_stacks.items():\n",
    "    print(f\"Sub-stack for period {period} has shape: {stack['images'].shape} with dates: {stack['dates']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6693734e-ada7-478e-b8a5-27c64c665839",
   "metadata": {},
   "source": [
    "### Firn Aquifer Detection Process\n",
    "\n",
    "The detection of firn aquifers relies on a methodology developed by (Brangers et al., 2020), which analyzes C-band radar backscatter signals, specifically focusing on the σ₀(HV) measurements in wet snow zones. Liquid water within the snow and firn layers significantly influences radar signal behavior, leading to lower σ₀ values during melt events. This is because liquid water absorbs and reflects much of the radar signal, reducing the penetration depth and causing strong decreases in σ₀ during summer melt. \n",
    "\n",
    "In areas where firn aquifers are typically found, such as the wet snow zone, strong summer melting saturates the firn, resulting in low summer σ₀. As the liquid water content decreases due to refreezing, σ₀(HV) values increase, as the radar signal penetrates deeper into the firn and encounters more volume scattering from the snow and firn. The σ₀(HV) time series at select locations within the wet snow zone is shown in Figure 1.\n",
    "\n",
    "![Figure 1: Time series](Data/Sentinel-1/Brangers.jpg)\n",
    "\n",
    "This figure is an example to illustrates the σ₀(HV) time series at select locations (in Greenland) within the wet snow zone (Brangers et al., 2020).The specific locations of the grid cells are indicated correspond to the following coordinates:\n",
    "\n",
    "- **Northwest Greenland**: \n",
    "  - Aquifer: 76.51°N, 61.75°E \n",
    "  - Non-aquifer: 76.61°N, 61.28°E \n",
    "\n",
    "- **Southeast Greenland**: \n",
    "  - Aquifer: 65.30°N, 41.65°E \n",
    "  - Non-aquifer: 65.51°N, 41.98°E \n",
    "\n",
    "- **South Greenland**: \n",
    "  - Aquifer: 61.77°N, 46.38°E \n",
    "  - Non-aquifer: 62.03°N, 46.27°E \n",
    "\n",
    "These time series allow us to analyze the differences in backscatter signals between aquifer and non-aquifer locations, providing insights into the seasonal dynamics of the firn aquifers.\n",
    "\n",
    "------\n",
    "\n",
    "The refreezing of liquid water in aquifer locations is generally delayed due to the insulating effects of autumn snow accumulation and the release of latent heat during refreezing. This results in a marked delay in the increase of σ₀(HV) throughout autumn compared to other areas where refreezing occurs sooner. Importantly, the radar may not directly sense the water table but can detect the slowdown in refreezing in the upper layers, which serves as a proxy for identifying aquifers.\n",
    "\n",
    "To effectively map firn aquifers, we employ a detection criterion based on the difference in σ₀(HV) between early autumn and late winter. Specifically, the reduction in σ₀(HV) during early autumn must exceed a threshold of 9.4 dB relative to the values observed near the end of winter. This criterion is derived from the observation that σ₀(HV) remains low in aquifer locations (due to persistent liquid water) and is relatively higher in non-aquifer areas (where refreezing occurs earlier). By applying this methodology, we can reliably identify and map firn aquifers across the studied regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fce2e0-75fc-49e7-aa7d-739c985e6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each annual stack, filter only the images from March of the year yyyy+1 (early autumn) \n",
    "# and October of the year yyyy (late winter) and calculate the pixel-wise averages. \n",
    "# The averages are saved in dedicated stacks and represented in georeferenced pcolor figures \n",
    "# with a color scale saturated in the range [-30; 0] dB. Additionally, the difference \n",
    "# between March yyyy+1 and October yyyy is calculated, which is useful for the application \n",
    "# of the Brangers et al. method, and is represented in the range [-10; 10] dB, \n",
    "# and also saved in a stack.\n",
    "\n",
    "# Open the GeoTIFF mask file using rasterio\n",
    "with rasterio.open('Data/Sentinel-1/maskAP.tif') as dataset:\n",
    "    maskAP = dataset.read(1)  # Read the first layer of the GeoTIFF file\n",
    "\n",
    "    # Get the affine transformation values to convert from pixel coordinates to geographic coordinates\n",
    "\n",
    "    transform_affine = dataset.transform\n",
    "\n",
    "# Extract the values of the x and y coordinates\n",
    "\n",
    "    cols, rows = np.meshgrid(np.arange(maskAP.shape[1]), np.arange(maskAP.shape[0]))\n",
    "    xs, ys = rasterio.transform.xy(transform_affine, rows, cols)\n",
    "\n",
    "# Flatten the coordinate arrays\n",
    "    xs_flat = np.array(xs).flatten()\n",
    "    ys_flat = np.array(ys).flatten()\n",
    "\n",
    "  # Transform the x and y coordinates into latitude and longitude\n",
    "    transformer = Transformer.from_crs(\"EPSG:3031\", \"EPSG:4326\", always_xy=True)\n",
    "    lon_flat, lat_flat = transformer.transform(xs_flat, ys_flat)\n",
    "\n",
    "# Reshape the latitude and longitude arrays\n",
    "\n",
    "    lon = np.array(lon_flat).reshape(maskAP.shape)\n",
    "    lat = np.array(lat_flat).reshape(maskAP.shape)\n",
    "\n",
    "# Visualization parameters\n",
    "vmin = -30  # Common lower limit for the first two plots\n",
    "vmax = 0    # Common upper limit for the first two plots\n",
    "diff_vmin = -10  # Lower limit for the difference\n",
    "diff_vmax = 10   # Upper limit for the difference\n",
    "\n",
    "\n",
    "# Initialize the stacks for the annual averages\n",
    "mean_october_stack = []\n",
    "mean_march_stack = []\n",
    "difference_stack = []\n",
    "annual_extremes = []\n",
    "\n",
    "# Iterate through the annual stacks\n",
    "for period, stack in annual_sub_stacks.items():\n",
    "    images = stack[\"images\"]\n",
    "    dates = pd.to_datetime(stack[\"dates\"])\n",
    "\n",
    "    # Initialize the lists to hold the October and March images\n",
    "    october_images = []\n",
    "    march_images = []\n",
    "\n",
    "    # Filter the images based on the months\n",
    "    for date, image in zip(dates, images):\n",
    "        if date.month == 10:  # Octoeber\n",
    "            october_images.append(image)\n",
    "        elif date.month == 3:  # March\n",
    "            march_images.append(image)\n",
    "\n",
    "   # Convert the lists into NumPy arrays\n",
    "    october_images = np.array(october_images)\n",
    "    march_images = np.array(march_images)\n",
    "\n",
    "  # Check if there are images for both months\n",
    "    if len(october_images) > 0 and len(march_images) > 0:\n",
    "       # Calculate the average for October and March, ignoring NaNs\n",
    "        mean_october = np.nanmean(october_images, axis=0)\n",
    "        mean_march = np.nanmean(march_images, axis=0)\n",
    "\n",
    "        # Calculate the difference (March - October)\n",
    "        difference = mean_march - mean_october\n",
    "\n",
    "        # Save the annual averages and extremes\n",
    "        mean_october_stack.append(mean_october)\n",
    "        mean_march_stack.append(mean_march)\n",
    "        difference_stack.append(difference)\n",
    "        annual_extremes.append((period, mean_october.min(), mean_october.max(), mean_march.min(), mean_march.max()))\n",
    "\n",
    "        # Plotting as a contour plot\n",
    "        plt.figure(figsize=(18, 6))\n",
    "\n",
    "        # Plot of the average - October\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.pcolormesh(lon, lat, np.ma.masked_array(mean_october, mask=np.isnan(maskAP)), cmap='viridis', vmin=vmin, vmax=vmax, shading='auto')\n",
    "        plt.title('Mean October Backscattering (dB)')\n",
    "        plt.colorbar(label='Backscattering (dB)')\n",
    "        plt.axis('off')\n",
    "\n",
    "  # Plot of the average - March\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.pcolormesh(lon, lat, np.ma.masked_array(mean_march, mask=np.isnan(maskAP)), cmap='viridis', vmin=vmin, vmax=vmax, shading='auto')\n",
    "        plt.title('Mean March Backscattering (dB)')\n",
    "        plt.colorbar(label='Backscattering (dB)')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot of the difference (March - October)\n",
    "        plt.subplot(1, 3, 3)\n",
    "        diff_masked = np.ma.masked_array(difference, mask=np.isnan(maskAP))\n",
    "        plt.pcolormesh(lon, lat, diff_masked, cmap='coolwarm', vmin=diff_vmin, vmax=diff_vmax, shading='auto')\n",
    "        plt.title('Difference (March - October)')\n",
    "        plt.colorbar(label='Difference (dB)')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f'Sub-stack Period: {period}')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No images for October or March in the period {period}.\")\n",
    "\n",
    "# Convert the stacks to NumPy arrays if necessary\n",
    "mean_october_stack = np.array(mean_october_stack)\n",
    "mean_march_stack = np.array(mean_march_stack)\n",
    "difference_stack = np.array(difference_stack)\n",
    "\n",
    "# Print the annual extremes\n",
    "for period, min_oct, max_oct, min_mar, max_mar in annual_extremes:\n",
    "    print(f\"Year: {period}, Mean October Range: ({min_oct}, {max_oct}), Mean March Range: ({min_mar}, {max_mar})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040cab1-89fd-429b-ba8f-2e3248157ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "treshold_brangers = -9.4  # Threshold for aquifer pixels (this can be edited)\n",
    "\n",
    "\n",
    "# Initialize the cumulative array to sum the aquifer pixels across all years\n",
    "# This array will have the same dimensions as the images (lat, lon)\n",
    "\n",
    "cumulative_aquifers = np.zeros_like(maskAP)\n",
    "\n",
    "# Iterate through the images of difference_stack and annual_extremes\n",
    "for i, (difference, year_info) in enumerate(zip(difference_stack, annual_extremes)):\n",
    "    period = year_info[0]  # Estrai l'anno di riferimento\n",
    "    \n",
    "    # Apply the maskAP to the difference images: where maskAP is NaN, difference also becomes NaN\n",
    "    masked_difference = np.where(np.isnan(maskAP), np.nan, difference)\n",
    "    \n",
    "   # Create a binary mask for aquifer pixels: 1 if difference < threshold_brangers, 0 otherwise\n",
    "    aquifers_mask = np.where(masked_difference < treshold_brangers, 1, 0)\n",
    "    \n",
    "# Apply the maskAP again to the binary mask to keep only the valid pixels\n",
    "    aquifers_mask = np.where(np.isnan(maskAP), np.nan, aquifers_mask)\n",
    "    \n",
    "   # Add to the cumulative sum\n",
    "    cumulative_aquifers += np.nan_to_num(aquifers_mask)  # Use nan_to_num to treat NaNs as 0\n",
    "    \n",
    "   # Plot the mask applied to the georeferenced map with lon and lat\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot the mask (maskAP) in the background\n",
    "    plt.pcolormesh(lon, lat, np.ma.masked_array(maskAP, mask=np.isnan(maskAP)), cmap='Greys', shading='auto')\n",
    "    \n",
    "# Overlay the aquifer pixels (1 where the threshold is exceeded, 0 where it is not)\n",
    "    plt.pcolormesh(lon, lat, aquifers_mask, cmap='Blues', shading='auto', alpha=0.6)\n",
    "    \n",
    "# Title with the reference year\n",
    "\n",
    "    plt.title(f\"Aquifer pixels during the year: {period}\")\n",
    "    plt.colorbar(label=\"Aquifer Pixels (1 = aquifer, 0 = not aquifer)\")\n",
    "    plt.axis('off')  # Remove the axes\n",
    "\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e60b4-a917-4319-8ffd-6514ba444959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After iterating through all the years, plot the cumulative sum of aquifer pixels\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Apply the maskAP to the cumulative sum: NaN pixels in maskAP remain NaN\n",
    "acquiferi_Brangers = np.where(np.isnan(maskAP), np.nan, cumulative_aquifers)\n",
    "\n",
    "# Plot the cumulative sum of aquifer pixels\n",
    "plt.pcolormesh(lon, lat, acquiferi_Brangers, cmap='coolwarm', shading='auto')\n",
    "\n",
    "# Add a colorbar indicating the number of years a pixel has been aquifer\n",
    "plt.colorbar(label='Number of years with aquifer pixels')\n",
    "\n",
    "plt.title('Total years in which the pixels have been aquifer (threshold: 9.4 dB)')\n",
    "plt.axis('off')  # Remove axes\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba34c7d1-0b96-43db-9c93-303bb8ce30d7",
   "metadata": {},
   "source": [
    "# Aquifer Pixel Analysis and Visualization\n",
    "\n",
    "This block of code analyzes and visualizes the presence of aquifer pixels across multiple thresholds using Sentinel-1 radar backscatter data. It iterates through a list of predefined thresholds (`soglie_brangers`) to identify aquifer pixels in the cumulative images, summing the occurrences of aquifer presence for each threshold. \n",
    "\n",
    "The first loop specifically calculates the cumulative sum of aquifer pixels for the threshold of -6 dB to determine the global maximum for the colorbar. For each threshold, a map is generated showing the cumulative years a pixel has been classified as an aquifer, with a corresponding colorbar to indicate the frequency of aquifer presence over the study period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762bcf96-a428-47d1-93c9-80d7974da355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of thresholds to iterate through\n",
    "brangers_thresholds = [6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "# Initialize a variable to store the global maximum for the colorbar\n",
    "global_max_acquifers = 0\n",
    "\n",
    "# First loop: calculate the image with threshold 6 to obtain the limits of the colorbar\n",
    "for threshold_i in brangers_thresholds:\n",
    "    # Initialize the cumulative array to sum the aquifer pixels across all years for the current threshold\n",
    "    cumulative_aquifers = np.zeros_like(maskAP)\n",
    "\n",
    "    # Iterate through the images of difference_stack and annual_extremes\n",
    "    for i, (difference, year_info) in enumerate(zip(difference_stack, annual_extremes)):\n",
    "        period = year_info[0]  # Extract the reference year\n",
    "        \n",
    "     # Apply the maskAP to the difference images: where maskAP is NaN, difference also becomes NaN\n",
    "        masked_difference = np.where(np.isnan(maskAP), np.nan, difference)\n",
    "        \n",
    "        # Create a binary mask for aquifer pixels: 1 if difference < threshold, 0 otherwise\n",
    "        aquifers_mask = np.where(masked_difference < -threshold_i, 1, 0)  # Note: difference < -threshold, since the threshold is negative\n",
    "        \n",
    "       # Apply the maskAP again to the binary mask to keep only the valid pixels\n",
    "        aquifers_mask = np.where(np.isnan(maskAP), np.nan, aquifers_mask)\n",
    "        \n",
    "        # Add to the cumulative sum only for valid pixels\n",
    "        cumulative_aquifers += np.nan_to_num(aquifers_mask)  # Use nan_to_num to treat NaNs as 0\n",
    "\n",
    "    # In the first loop (threshold 6), find the maximum of the cumulative aquifer pixels and store it\n",
    "    if threshold_i == 6:\n",
    "        global_max_acquifers = np.nanmax(cumulative_aquifers)\n",
    "\n",
    "    # Plotting for each threshold\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Apply the maskAP to the cumulative sum: NaN pixels in maskAP remain NaN\n",
    "    cumulative_aquifers_masked = np.where(np.isnan(maskAP), np.nan, cumulative_aquifers)\n",
    "\n",
    "   # Plot the cumulative sum of aquifer pixels\n",
    "    plot = plt.pcolormesh(lon, lat, cumulative_aquifers_masked, cmap='Purples', shading='auto', vmin=0, vmax=global_max_acquifers)\n",
    "    \n",
    "    # Add a colorbar indicating the number of years a pixel has been aquifer\n",
    "    plt.colorbar(plot, label='Number of years with aquifer pixels')\n",
    "    \n",
    " # Title that includes the value of the threshold used\n",
    "\n",
    "    plt.title(f'Total years in which the pixels have been aquifer (Threshold = -{threshold_i} dB)')\n",
    "    plt.axis('off')  # Remove axes\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f4337f-4c4b-49ae-9b53-861d32cae763",
   "metadata": {},
   "source": [
    "Now we analyze the number of aquifer pixels for each year in the `difference_stack` dataset across a range of thresholds (from 6 to 14 dB in increments of 0.1 dB). For each year, it creates a binary mask indicating the presence of aquifer pixels based on the condition that the masked difference is less than the negative threshold.\n",
    "\n",
    "The code initializes a list to store the count of pixels exceeding each threshold, iterates through the specified threshold range, and applies the necessary masks to isolate valid pixels. Finally, it visualizes the relationship between the threshold values and the number of positive aquifer pixels for each year, providing insights into how varying thresholds affect pixel classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51eceb7-f005-4b49-b6de-bfcb6d5c37e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold range from 6 to 14 dB with a step of 0.1\n",
    "\n",
    "brangers_thresholds = np.arange(6, 14.1, 0.1)\n",
    "\n",
    "# # Iterate through all the years in the difference_stack\n",
    "for i, difference_year in enumerate(difference_stack):\n",
    "    # Check the length of annual_extremes[i] and assign accordingly\n",
    "\n",
    "    if isinstance(annual_extremes[i], (list, tuple)) and len(annual_extremes[i]) == 2:\n",
    "        start_year, end_year = annual_extremes[i]\n",
    "        year_label = f\"{start_year} - {end_year}\"\n",
    "    else:\n",
    "        year_label = f\"{annual_extremes[i]}\"  # If there is only a single year\n",
    "\n",
    "        \n",
    "    # Initialize a list to store the number of pixels that exceed the threshold\n",
    "    pixel_count_above_threshold = []\n",
    "    \n",
    "    # Iterate through the thresholds\n",
    "    for threshold_i in brangers_thresholds:\n",
    "       # Apply the maskAP: where maskAP is NaN, difference also becomes NaN\n",
    "        masked_difference = np.where(np.isnan(maskAP), np.nan, difference_year)\n",
    "        \n",
    "        # Create a binary mask for aquifer pixels: 1 if difference < threshold, 0 otherwise\n",
    "        aquifers_mask = np.where(masked_difference < -threshold_i, 1, 0)  # Note: difference < -threshold\n",
    "\n",
    "        \n",
    "        # Apply the maskAP to the binary mask to keep only the valid pixels\n",
    "        aquifers_mask = np.where(np.isnan(maskAP), np.nan, aquifers_mask)\n",
    "        \n",
    "        # Count the aquifer pixels (value 1) ignoring NaNs\n",
    "        pixel_count = np.nansum(aquifers_mask)\n",
    "        \n",
    "    # Add the count to the list\n",
    "        pixel_count_above_threshold.append(pixel_count)\n",
    "    \n",
    "   # Plotting: Number of pixels as a function of the threshold for the current year\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(brangers_thresholds, pixel_count_above_threshold, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Number of positive pixels as a function of the threshold (Year {year_label})')\n",
    "    plt.xlabel('Brangers Threshold (dB)')\n",
    "    plt.ylabel('Number of positive pixels')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f09a5-16c3-4a86-95c2-216bd9982da7",
   "metadata": {},
   "source": [
    "## VALIDATION using OIB data\n",
    "\n",
    "The Operation IceBridge (OIB) mission, conducted by NASA, aims to monitor changes in polar ice and glaciers by bridging the gap between the ICESat missions. Utilizing a combination of airborne instruments, IceBridge provides critical data on ice thickness, surface elevation, and other geophysical parameters across the Arctic and Antarctic regions. The mission has contributed to a better understanding of the dynamics of ice sheets, including seasonal changes, and has been instrumental in validating satellite-based observations. Data from IceBridge, particularly in relation to firn aquifers, serves as a valuable reference to assess the accuracy of remote sensing techniques employed in our analysis. The findings from Miege et al. 2016 highlight the significance of these datasets for improving our understanding of the cryosphere and the impacts of climate change.\n",
    "These data, as provided by Miege et al.2020 (https://www.usap-dc.org/view/dataset/601390), are used for validating the presence of aquifers on the Wilkins ice Shelf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a636bad-31bc-41fa-b321-f2d386b14805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OIB points \n",
    "\n",
    "# Path to the CSV file\n",
    "file_path = 'Data/Sentinel-1/OIB_Wilkins.csv'\n",
    "\n",
    "# Select only the columns for LATITUDE, LONGITUDE, and DEPTH\n",
    "columns_to_import = ['LATITUDE', 'LONGITUDE', 'DEPTH (m)']\n",
    "\n",
    "# Import the data\n",
    "oib_data_subset = pd.read_csv(file_path, usecols=columns_to_import)\n",
    "\n",
    "# Show the first few rows\n",
    "print(oib_data_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed95c14d-2f6e-4460-a86e-6d37043a3b7a",
   "metadata": {},
   "source": [
    "### Visualization of Depth Data from Operation IceBridge\n",
    "\n",
    "This code block visualizes depth data from the Wilkins Ice Shelf, collected during the Operation IceBridge mission. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c51dc-20ea-4f19-857b-c513255be486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# file_path = 'OIB_Wilkins.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract latitude, longitude, and depth\n",
    "lat_OIB = data['LATITUDE']\n",
    "lon_OIB = data['LONGITUDE']\n",
    "depth = data['DEPTH (m)']\n",
    "\n",
    "# Remove NaN values from lat_OIB, lon_OIB, and depth\n",
    "valid_data = data.dropna(subset=['LATITUDE', 'LONGITUDE', 'DEPTH (m)'])\n",
    "lat_OIB = valid_data['LATITUDE']\n",
    "lon_OIB = valid_data['LONGITUDE']\n",
    "depth = valid_data['DEPTH (m)']\n",
    "\n",
    "# Debugging: Print the number of valid points\n",
    "print(f'Number of valid data points: {len(lat_OIB)}')\n",
    "\n",
    "# Create a Basemap for the Antarctic Peninsula (Wilkins Ice Shelf area)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Use Plate Carrée projection (cylindrical) for clearer, less distorted projection\n",
    "m = Basemap(projection='cyl', llcrnrlat=-72, urcrnrlat=-68, llcrnrlon=-75, urcrnrlon=-70, resolution='i')\n",
    "\n",
    "# Draw coastlines, countries, and parallels/meridians\n",
    "m.drawcoastlines()\n",
    "m.drawparallels(range(-90, -60, 2), labels=[1, 0, 0, 0])\n",
    "m.drawmeridians(range(-180, 180, 5), labels=[0, 0, 0, 1])\n",
    "\n",
    "# Convert lat_OIB/lon_OIB to map projection coordinates\n",
    "x, y = m(lon_OIB.values, lat_OIB.values)\n",
    "\n",
    "# Debugging: Print the x and y coordinates\n",
    "print(f'x coordinates: {x}')\n",
    "print(f'y coordinates: {y}')\n",
    "\n",
    "# Plot the data points with depth represented as color\n",
    "# Increase the marker size for better visibility\n",
    "sc = m.scatter(x, y, c=depth, cmap='viridis', marker='o', edgecolor='k', s=40)\n",
    "\n",
    "# Add colorbar to represent depth\n",
    "cb = plt.colorbar(sc)\n",
    "cb.set_label('Depth (m)')\n",
    "\n",
    "# Add title\n",
    "plt.title('Wilkins Ice Shelf Depth Representation - Antarctic Peninsula')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7abe28-59d6-4bd8-ba37-32d25f83c4aa",
   "metadata": {},
   "source": [
    "# Depth Interpolation from Operation IceBridge Data\n",
    "\n",
    "This code processes depth data collected from the Operation IceBridge mission. It utilizes latitude and longitude coordinates to interpolate depth values onto a predefined grid. The resulting depth data is then visualized over a specified geographical area, providing insights into the ice shelf dynamics in the Wilkins region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15acc12-fe69-4b9d-b72f-8812be0f040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract latitude, longitude, and depth\n",
    "lat_OIB = data['LATITUDE']\n",
    "lon_OIB = data['LONGITUDE']\n",
    "depth = data['DEPTH (m)']\n",
    "\n",
    "# Remove NaN values from lat_OIB, lon_OIB, and depth\n",
    "valid_data = data.dropna(subset=['LATITUDE', 'LONGITUDE', 'DEPTH (m)'])\n",
    "lat_OIB = valid_data['LATITUDE'].values\n",
    "lon_OIB = valid_data['LONGITUDE'].values\n",
    "depth = valid_data['DEPTH (m)'].values\n",
    "\n",
    "# Assuming maskAP, lat, and lon are already defined matrices (from previous work)\n",
    "# Let's assume lat and lon are 2D arrays corresponding to the geographical area covered by maskAP\n",
    "\n",
    "# Interpolate depth values onto the grid defined by lat and lon\n",
    "interpolated_depth = griddata((lon_OIB, lat_OIB), depth, (lon, lat), method='linear')\n",
    "\n",
    "# Mask the interpolated depth using maskAP\n",
    "masked_depth = np.where(np.isnan(maskAP), np.nan, interpolated_depth)\n",
    "\n",
    "# Plot the interpolated depth over the maskAP\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Use Plate Carrée projection (cylindrical) for clearer, less distorted projection\n",
    "m = Basemap(projection='cyl', llcrnrlat=-72, urcrnrlat=-68, llcrnrlon=-75, urcrnrlon=-70, resolution='i')\n",
    "\n",
    "# Draw coastlines and parallels/meridians\n",
    "m.drawcoastlines()\n",
    "m.drawparallels(range(-90, -60, 2), labels=[1, 0, 0, 0])\n",
    "m.drawmeridians(range(-180, 180, 5), labels=[0, 0, 0, 1])\n",
    "\n",
    "# Plot the masked depth\n",
    "c = m.contourf(lon, lat, masked_depth, cmap='viridis', levels=100)\n",
    "\n",
    "# Add colorbar\n",
    "cb = plt.colorbar(c)\n",
    "cb.set_label('Depth (m)')\n",
    "\n",
    "# Add title\n",
    "plt.title('Interpolated OIB Depth over MaskAP - Wilkins Area')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d3aaf-8388-46f5-bb25-c56d5c9dd1d7",
   "metadata": {},
   "source": [
    "# Visualization of Aquifer Years in the Wilkins Area\n",
    "\n",
    "Now we compute and visualize the number of years each pixel in the Wilkins area has been classified as an aquifer based on difference data. A predefined threshold is applied to identify aquifer pixels. The results are plotted on a map, with overlay points from the Operation IceBridge (OIB) dataset, providing insights into the spatial distribution of aquifer presence in relation to the OIB data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d2c83-5657-4234-ba11-1753856bcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming maskAP and difference_stack are already defined and contain the relevant data\n",
    "# Here, difference_stack is assumed to have the years of difference data\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 9.4\n",
    "\n",
    "# Initialize the cumulative array to sum the aquifer pixels across all years for the current threshold\n",
    "cumulative_aquifers = np.zeros_like(maskAP)\n",
    "\n",
    "# Iterate through the images of difference_stack and annual_extremes\n",
    "for i, (difference, year_info) in enumerate(zip(difference_stack, annual_extremes)):\n",
    "        \n",
    "        # Apply the maskAP to the difference images: where maskAP is NaN, difference also becomes NaN\n",
    "        masked_difference = np.where(np.isnan(maskAP), np.nan, difference)\n",
    "        \n",
    "        # Create a binary mask for aquifer pixels: 1 if difference < threshold, 0 otherwise\n",
    "        aquifers_mask = np.where(masked_difference < -threshold, 1, 0)  # Note: difference < -threshold, since the threshold is negative\n",
    "        \n",
    "        # Apply the maskAP again to the binary mask to keep only the valid pixels\n",
    "        aquifers_mask = np.where(np.isnan(maskAP), np.nan, aquifers_mask)\n",
    "        \n",
    "       # Add to the cumulative sum only for valid pixels\n",
    "        cumulative_aquifers += np.nan_to_num(aquifers_mask)  # Use nan_to_num to treat NaNs as 0\n",
    "\n",
    "\n",
    "# Mask the years_aquifer using maskAP\n",
    "years_aquifer_masked = np.where(np.isnan(maskAP), np.nan, cumulative_aquifers)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Use Plate Carrée projection (cylindrical) for clearer, less distorted projection\n",
    "m = Basemap(projection='cyl', llcrnrlat=-72, urcrnrlat=-68, llcrnrlon=-75, urcrnrlon=-70, resolution='i')\n",
    "\n",
    "# Draw coastlines and parallels/meridians\n",
    "m.drawcoastlines()\n",
    "m.drawparallels(range(-90, -60, 2), labels=[1, 0, 0, 0])\n",
    "m.drawmeridians(range(-180, 180, 5), labels=[0, 0, 0, 1])\n",
    "\n",
    "# Plot the years_aquifer_masked\n",
    "c = m.contourf(lon, lat, years_aquifer_masked, cmap='BuPu', levels=np.arange(0, cumulative_aquifers.max()+1), alpha=0.7)\n",
    "\n",
    "# Overlay the OIB points\n",
    "x_OIB, y_OIB = m(lon_OIB, lat_OIB)\n",
    "m.scatter(x_OIB, y_OIB, color='red', marker='o', label='OIB Points', edgecolor='k', zorder=5)\n",
    "\n",
    "# Add colorbar for years aquifer\n",
    "cb = plt.colorbar(c)\n",
    "cb.set_label('Number of Aquifer Years')\n",
    "\n",
    "# Add title\n",
    "plt.title('Number of Aquifer Years - Wilkins Area with OIB Points')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18273622-4f9c-4301-b676-ea5c4038599a",
   "metadata": {},
   "source": [
    "# Climate Data Integration from RACMO2.3\n",
    "\n",
    "This script incorporates regional climate model data from RACMO2.3 at a 2 × 2 km resolution for the Antarctic region with the aim of offering a visual comparison of the satellite detected PFA and the climate conditions associated with their occurrence. The dataset includes key climate variables such as accumulation and melt data, which are crucial for identifying regions with potential PFA. These conditions, typically associated with moderate to high surface melt and elevated accumulation rates, are downscaled from the original 27 × 27 km resolution of RACMO2.3p2 (Noël et al., 2023). For Greenland, 1 × 1 km resolution data is provided, downscaled from the 5.5 × 5.5 km RACMO2.3p2 model output (Noël et al., 2019).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd788c-c56b-4cff-a78f-a2b34fa4c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading and Georeferencing Climate Data\n",
    "\n",
    "#This code reads two TIFF files containing melt and accumulation data for the Antarctic Peninsula, \n",
    "#converts their pixel coordinates into geographic coordinates (latitude and longitude), \n",
    "#and extracts key information such as dimensions and resolution. The transformation from the projected coordinate system EPSG:3031 to \n",
    "#geographic coordinates EPSG:4326 is performed using the `pyproj` library. \n",
    "#The script outputs the dimensions and resolution of both datasets for further analysis.\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
    "from pyproj import Transformer\n",
    "\n",
    "# Paths to the TIFF files (update these if needed)\n",
    "melt_tif_path = 'Data/Sentinel-1/AP_melt.tif'\n",
    "accumulation_tif_path = 'Data/Sentinel-1/AP_accumulation.tif'\n",
    "\n",
    "# Create a transformer to convert from EPSG:3031 to EPSG:4326\n",
    "transformer = Transformer.from_crs(\"EPSG:3031\", \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "# Import the AP_melt.tif and AP_accumulation.tif files\n",
    "with rasterio.open(melt_tif_path) as melt_src, rasterio.open(accumulation_tif_path) as accumulation_src:\n",
    "    \n",
    "    # Extract the dimensions (n_image, row, col) for both files\n",
    "    melt_shape = (melt_src.count, melt_src.height, melt_src.width)  # (bands, rows, cols)\n",
    "    accumulation_shape = (accumulation_src.count, accumulation_src.height, accumulation_src.width)\n",
    "\n",
    "    # Extract the georeferencing information (Affine transformation)\n",
    "    melt_transform = melt_src.transform\n",
    "    accumulation_transform = accumulation_src.transform\n",
    "    \n",
    "    # Extract resolution (deduced from the affine transform)\n",
    "    melt_resolution = (melt_transform[0], -melt_transform[4])  # (pixel width, pixel height)\n",
    "    accumulation_resolution = (accumulation_transform[0], -accumulation_transform[4])\n",
    "\n",
    "    # Create grids for x and y coordinates (centered coordinates for each pixel)\n",
    "    cols, rows = np.meshgrid(np.arange(melt_src.width), np.arange(melt_src.height))\n",
    "\n",
    "    # Convert the row/col indices to georeferenced x/y using the transform\n",
    "    x_AP, y_AP = rasterio.transform.xy(melt_transform, rows, cols)\n",
    "\n",
    "    # Convert the georeferenced x/y to lat/lon using the pyproj Transformer\n",
    "    lon_AP, lat_AP = transformer.transform(np.array(x_AP), np.array(y_AP))\n",
    "\n",
    "    # Read the melt data\n",
    "    melt_data = melt_src.read()  # Read all bands\n",
    "\n",
    "    # Read the accumulation data\n",
    "    accumulation_data = accumulation_src.read()  # Read all bands\n",
    "\n",
    "# Print the dimensions and resolution\n",
    "print(f\"Melt TIFF dimensions: {melt_shape}\")\n",
    "print(f\"Accumulation TIFF dimensions: {accumulation_shape}\")\n",
    "print(f\"Melt TIFF resolution (width, height): {melt_resolution}\")\n",
    "print(f\"Accumulation TIFF resolution (width, height): {accumulation_resolution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88604e8-12c0-4319-a7ea-685ad4ad9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojection and plotting of melt flux\n",
    "\n",
    "# Start from the year 2017 and iterate over each year of melt data\n",
    "start_year = 2017\n",
    "\n",
    "# Initialize matrices to store reprojected melt data\n",
    "melt_reprojected = np.zeros((melt_data.shape[0], lat.shape[0], lat.shape[1]))\n",
    "\n",
    "# Loop over the melt data for each year\n",
    "for i in range(melt_data.shape[0]):\n",
    "    # Get the melt data for the current year\n",
    "    melt_current = melt_data[i, :, :]\n",
    "    \n",
    "    # Flatten the lat_AP, lon_AP, and melt data for interpolation\n",
    "    lat_AP_flat = lat_AP.flatten()\n",
    "    lon_AP_flat = lon_AP.flatten()\n",
    "    melt_flat = melt_current.flatten()\n",
    "    \n",
    "    # Interpolate melt data onto the new lat/lon grid\n",
    "    melt_interpolated = griddata(\n",
    "        (lon_AP_flat, lat_AP_flat),  # Original lon/lat coordinates\n",
    "        melt_flat,                   # Original melt data values\n",
    "        (lon, lat),        # New grid of lon/lat to interpolate onto\n",
    "        method='linear'              # Interpolation method\n",
    "    )\n",
    "\n",
    "    # Store the reprojected melt flux data in the matrix\n",
    "    melt_reprojected[i, :, :] = melt_interpolated\n",
    "    \n",
    "    # Plot the reprojected melt data using Basemap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Create the Basemap for the Antarctic Peninsula with the given limits\n",
    "    m = Basemap(projection='cyl', \n",
    "                llcrnrlat=-78, urcrnrlat=-58, \n",
    "                llcrnrlon=-80, urcrnrlon=-50, \n",
    "                resolution='i')\n",
    "\n",
    "    # Draw coastlines, parallels, and meridians\n",
    "    m.drawcoastlines()\n",
    "    m.drawparallels(np.arange(-90, -50, 5), labels=[1, 0, 0, 0])\n",
    "    m.drawmeridians(np.arange(-180, 180, 10), labels=[0, 0, 0, 1])\n",
    "\n",
    "    # Plot the interpolated melt data on the Basemap\n",
    "    x, y = m(lon, lat)\n",
    "    cs = m.contourf(x, y, melt_interpolated, cmap='coolwarm', levels=np.linspace(0, 800, 100))\n",
    "\n",
    "    # Add a colorbar with the label for melt in mm w.e./year\n",
    "    cb = plt.colorbar(cs)\n",
    "    cb.set_label('mm w.e./year')\n",
    "\n",
    "    # Add the title with the corresponding year\n",
    "    plt.title(f'Melt Flux in AP for the year {start_year + i}')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d973e7fc-0842-404a-9049-85d48e92e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojection and plotting of accumulation\n",
    "\n",
    "# Start from the year 2017 and iterate over each year of accumulation data\n",
    "start_year = 2017\n",
    "\n",
    "# Initialize matrices to store reprojected accumulation data\n",
    "accumulation_reprojected = np.zeros((accumulation_data.shape[0], lat.shape[0], lat.shape[1]))\n",
    "\n",
    "# Loop over the accumulation data for each year\n",
    "for i in range(accumulation_data.shape[0]):\n",
    "    # Get the accumulation data for the current year\n",
    "    accumulation_current = accumulation_data[i, :, :]\n",
    "    \n",
    "    # Flatten the lat_AP, lon_AP, and accumulation data for interpolation\n",
    "    lat_AP_flat = lat_AP.flatten()\n",
    "    lon_AP_flat = lon_AP.flatten()\n",
    "    accumulation_flat = accumulation_current.flatten()\n",
    "    \n",
    "    # Interpolate accumulation data onto the new lat/lon grid\n",
    "    accumulation_interpolated = griddata(\n",
    "        (lon_AP_flat, lat_AP_flat),  # Original lon/lat coordinates\n",
    "        accumulation_flat,           # Original accumulation data values\n",
    "        (lon, lat),                  # New grid of lon/lat to interpolate onto\n",
    "        method='linear'              # Interpolation method\n",
    "    )\n",
    "\n",
    "    # Store the reprojected accumulation data in the matrix\n",
    "    accumulation_reprojected[i, :, :] = accumulation_interpolated\n",
    "    \n",
    "    # Plot the reprojected accumulation data using Basemap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Create the Basemap for the Antarctic Peninsula with the given limits\n",
    "    m = Basemap(projection='cyl', \n",
    "                llcrnrlat=-78, urcrnrlat=-58, \n",
    "                llcrnrlon=-80, urcrnrlon=-50, \n",
    "                resolution='i')\n",
    "\n",
    "    # Draw coastlines, parallels, and meridians\n",
    "    m.drawcoastlines()\n",
    "    m.drawparallels(np.arange(-90, -50, 5), labels=[1, 0, 0, 0])\n",
    "    m.drawmeridians(np.arange(-180, 180, 10), labels=[0, 0, 0, 1])\n",
    "\n",
    "    # Plot the interpolated accumulation data on the Basemap\n",
    "    x, y = m(lon, lat)\n",
    "    cs = m.contourf(x, y, accumulation_interpolated, cmap='coolwarm', levels=np.linspace(0, 2000, 100))\n",
    "\n",
    "    # Add a colorbar with the label for accumulation in mm w.e./year\n",
    "    cb = plt.colorbar(cs)\n",
    "    cb.set_label('mm w.e./year')\n",
    "\n",
    "    # Add the title with the corresponding year\n",
    "    plt.title(f'Accumulation in AP for the year {start_year + i}')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a4a0f5-b553-408e-bf80-e917c198a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-year average values: plot\n",
    "\n",
    "# Calculate the mean of reprojected melt and accumulation data\n",
    "mean_melt_reprojected = np.mean(melt_reprojected, axis=0)\n",
    "mean_accumulation_reprojected = np.mean(accumulation_reprojected, axis=0)\n",
    "\n",
    "# Create a subplot to visualize mean melt and accumulation\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot for mean melt\n",
    "melt_map = Basemap(projection='cyl', llcrnrlat=-78, urcrnrlat=-58, llcrnrlon=-80, urcrnrlon=-50, resolution='i', ax=axs[0])\n",
    "melt_map.drawcoastlines()\n",
    "melt_map.drawparallels(np.arange(-90, -50, 5), labels=[1, 0, 0, 0])\n",
    "melt_map.drawmeridians(np.arange(-180, 180, 10), labels=[0, 0, 0, 1])\n",
    "\n",
    "melt_cs = melt_map.contourf(lon, lat, mean_melt_reprojected, cmap='coolwarm', levels=np.linspace(0, 800, 100))\n",
    "\n",
    "# Add colorbar for melt\n",
    "melt_cb = plt.colorbar(melt_cs, ax=axs[0])\n",
    "melt_cb.set_label('Melt Flux (mm w.e./year)')\n",
    "axs[0].set_title(f'Melt Flux: Mean over 2017-2021')\n",
    "\n",
    "# Plot for mean accumulation\n",
    "accumulation_map = Basemap(projection='cyl', llcrnrlat=-78, urcrnrlat=-58, llcrnrlon=-80, urcrnrlon=-50, resolution='i', ax=axs[1])\n",
    "accumulation_map.drawcoastlines()\n",
    "accumulation_map.drawparallels(np.arange(-90, -50, 5), labels=[1, 0, 0, 0])\n",
    "accumulation_map.drawmeridians(np.arange(-180, 180, 10), labels=[0, 0, 0, 1])\n",
    "\n",
    "accumulation_cs = accumulation_map.contourf(lon, lat,  mean_accumulation_reprojected, cmap='coolwarm', levels=np.linspace(0, 2000, 100))\n",
    "\n",
    "# Add colorbar for accumulation\n",
    "accumulation_cb = plt.colorbar(accumulation_cs, ax=axs[1])\n",
    "accumulation_cb.set_label('Accumulation (mm w.e./year)')\n",
    "axs[1].set_title(f'Accumulation: Mean over 2017-2021')\n",
    "\n",
    "# Set a main title for the subplot\n",
    "plt.suptitle('Melt and Accumulation Mean Values 2017-2021', fontsize=16)\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6b81f-4611-4395-8b3b-238932306ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of S1 PFA with pixels of high melt flux and high accumulation\n",
    "\n",
    "# Assume mean_melt_reprojected, mean_accumulation_reprojected, and acquiferi_Brangers are already defined\n",
    "\n",
    "# Define thresholds for masking\n",
    "melt_threshold = 200\n",
    "accumulation_threshold = 400\n",
    "\n",
    "# Create a mask where mean melt is greater than the threshold and mean accumulation is greater than the threshold\n",
    "mask = (mean_melt_reprojected > melt_threshold) & (mean_accumulation_reprojected > accumulation_threshold)\n",
    "\n",
    "# Apply the mask to acquiferi_Brangers\n",
    "# Assuming acquiferi_Brangers has the same shape as mean_melt_reprojected and mean_accumulation_reprojected\n",
    "masked_acquiferi = np.where(mask, acquiferi_Brangers, np.nan)  # Use NaN to hide masked values in the plot\n",
    "\n",
    "# Create a Basemap for the Antarctic Peninsula\n",
    "plt.figure(figsize=(10, 8))\n",
    "m = Basemap(projection='cyl', \n",
    "             llcrnrlat=-78, urcrnrlat=-58, \n",
    "             llcrnrlon=-80, urcrnrlon=-50, \n",
    "             resolution='i')\n",
    "\n",
    "# Draw coastlines, parallels, and meridians\n",
    "m.drawcoastlines()\n",
    "m.drawparallels(np.arange(-90, -50, 5), labels=[1, 0, 0, 0])\n",
    "m.drawmeridians(np.arange(-180, 180, 10), labels=[0, 0, 0, 1])\n",
    "\n",
    "# Plot the masked aquifers data\n",
    "cs = m.contourf(lon, lat, masked_acquiferi, cmap='coolwarm', levels=np.linspace(np.nanmin(masked_acquiferi), np.nanmax(masked_acquiferi), 100))\n",
    "\n",
    "# Add colorbar with label\n",
    "cb = plt.colorbar(cs)\n",
    "cb.set_label('S1 PFA (years)')\n",
    "\n",
    "# Add title\n",
    "plt.title('Aquifers According to S1 for High Melt and Accumulation Points')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea43959-ae78-4cd4-8e76-40e47f0ecbed",
   "metadata": {},
   "source": [
    "# End of the guided topic.\n",
    "\n",
    "This marks the conclusion of the guided section of this tutorial.\n",
    "Please analyze the results obtained and find a way to present them to your colleagues during the final presentation. You may use all the tools suggested in this guided part, but feel free to add your comments, analyses, or further ideas for additional investigations.\n",
    "\n",
    "Once you have commented on the results obtained in Antarctica, you can proceed with aquifer detection in Greenland using the same method outlined here. You will find a folder containing all the necessary data:\n",
    "a main folder with mask, accumulation, and melt, as well as a subfolder with individual years for Sentinel in .tif and .csv formats. The individual years for S1 are 2019, 2020, 2021, 2022, and 2023, while RACMO (melt and accumulation) is available for 2019, 2020, 2021, and 2022.\n",
    "\n",
    "Since we are now in a different hemisphere from Antarctica, the definition of 'year' and the implementation of the Brangers thresholds used in our method will need to be adjusted accordingly. As an approximation, everything can be shifted by six months. Please refer to Brangers 2020 for further details.\n",
    "\n",
    "Additionally, note that the reference system for the Greenland data is EPSG:5938, with a 4 km grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725935a4-5ccd-4db5-a460-a4f4008afb61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
